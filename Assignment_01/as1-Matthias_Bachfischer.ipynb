{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Preprocessing and Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Name: Matthias Bachfischer\n",
    "\n",
    "Student ID: 1133751"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Info\n",
    "\n",
    "<b>Due date</b>: Sunday, 28 March 2021 5pm\n",
    "\n",
    "<b>Submission method</b>: Canvas submission\n",
    "\n",
    "<b>Submission materials</b>: completed copy of this iPython notebook\n",
    "\n",
    "<b>Late submissions</b>: -10% per day (both week and weekend days counted)\n",
    "\n",
    "<b>Marks</b>: 9% of mark for class (with 8% on correctness + 1% on quality and efficiency of your code)\n",
    "\n",
    "<b>Materials</b>: See [Using Jupyter Notebook and Python page](https://canvas.lms.unimelb.edu.au/courses/121115/pages/using-jupyter-notebook-and-python?module_item_id=2681264) on Canvas (under Modules>Resources) for information on the basic setup required for this class, including an iPython notebook viewer and the python packages NLTK, Numpy, Scipy, Matplotlib, Scikit-Learn, and Gensim. We recommend installing all the data for NLTK, since you will need various parts of it to complete this assignment. You can also use any Python built-in packages, but do not use any other 3rd party packages (the packages listed above are all fine to use); if your iPython notebook doesn't run on the marker's machine, you will lose marks. <b> You should use Python 3</b>.  \n",
    "\n",
    "To familiarize yourself with NLTK, here is a free online book:  Steven Bird, Ewan Klein, and Edward Loper (2009). <a href=http://nltk.org/book>Natural Language Processing with Python</a>. O'Reilly Media Inc. You may also consult the <a href=https://www.nltk.org/api/nltk.html>NLTK API</a>.\n",
    "\n",
    "<b>Evaluation</b>: Your iPython notebook should run end-to-end without any errors in a reasonable amount of time, and you must follow all instructions provided below, including specific implementation requirements and instructions for what needs to be printed (please avoid printing output we don't ask for). You should edit the sections below where requested, but leave the rest of the code as is. You should leave the output from running your code in the iPython notebook you submit, to assist with marking. The amount each question is worth is explicitly given. \n",
    "\n",
    "You will be marked not only on the correctness of your methods, but also the quality and efficency of your code: in particular, you should be careful to use Python built-in functions and operators when appropriate and pick descriptive variable names that adhere to <a href=\"https://www.python.org/dev/peps/pep-0008/\">Python style requirements</a>. If you think it might be unclear what you are doing, you should comment your code to help the marker make sense of it.\n",
    "\n",
    "<b>Updates</b>: Any major changes to the assignment will be announced via Canvas. Minor changes and clarifications will be announced on the discussion board; we recommend you check it regularly.\n",
    "\n",
    "<b>Academic misconduct</b>: For most people, collaboration will form a natural part of the undertaking of this homework, and we encourge you to discuss it in general terms with other students. However, this ultimately is still an individual task, and so reuse of code or other instances of clear influence will be considered cheating. We will be checking submissions for originality and will invoke the University‚Äôs <a href=\"http://academichonesty.unimelb.edu.au/policy.html\">Academic Misconduct policy</a> where inappropriate levels of collusion or plagiarism are deemed to have taken place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this homework, you'll be working with a collection tweets. The task is to predict the geolocation (country) where the tweet comes from. This homework involves writing code to preprocess data and perform text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing (4 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions**: Download the data (as1-data.json) from Canvas and put it in the same directory as this iPython notebook. Run the code below to load the json data. This produces two objects, `x` and `y`, which contains a list of  tweets and corresponding country labels (it uses the standard [2 letter country code](https://www.iban.com/country-codes)) respectively. **No implementation is needed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets = 943\n",
      "Number of labels = 943\n",
      "\n",
      "Samples of data:\n",
      "Country = us \tTweet = @Addictd2Success thx u for following\n",
      "Country = us \tTweet = Let's just say, if I were to ever switch teams, Khalesi would be top of the list. #girlcrush\n",
      "Country = ph \tTweet = Taemin jonghyun!!! Your birits make me go~ http://t.co/le8z3dntlA\n",
      "Country = id \tTweet = depart.senior üëª rapat perdana (with Nyayu, Anita, and 8 others at Ruang Aescullap FK Unsri Madang) ‚Äî https://t.co/swRALlNkrQ\n",
      "Country = ph \tTweet = Done with internship with this pretty little lady! Ôòä (@ Metropolitan Medical Center w/ 3 others) [pic]: http://t.co/1qH61R1t5r\n",
      "Country = gb \tTweet = Wow just Boruc's clanger! Haha Sunday League stuff that, Giroud couldn't believe his luck! #clown\n",
      "Country = my \tTweet = I'm at Sushi Zanmai (Petaling Jaya, Selangor) w/ 5 others http://t.co/bcNobykZ\n",
      "Country = us \tTweet = Mega Fest!!!! Its going downüôèüôå  @BishopJakes\n",
      "Country = gb \tTweet = @EllexxxPharrell wow love the pic babe xx\n",
      "Country = us \tTweet = You have no clue how much you hurt me\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "data = json.load(open(\"as1-data.json\"))\n",
    "for k, v in data.items():\n",
    "    x.append(k)\n",
    "    y.append(v)\n",
    "    \n",
    "print(\"Number of tweets =\", len(x))\n",
    "print(\"Number of labels =\", len(y))\n",
    "print(\"\\nSamples of data:\")\n",
    "for i in range(10):\n",
    "    print(\"Country =\", y[i], \"\\tTweet =\", x[i])\n",
    "    \n",
    "assert(len(x) == 943)\n",
    "assert(len(y) == 943)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 (1.0 mark)\n",
    "\n",
    "**Instructions**: Next we need to preprocess the collected tweets to create a bag-of-words representation. The preprocessing steps required here are: (1) tokenize each tweet into individual word tokens (using NLTK `TweetTokenizer`); (2) lowercase all words; (3) remove any word that does not contain any English alphabets (e.g. {_hello_, _#okay_, _abc123_} would be kept, but not {_123_, _!!_}) and (4) remove stopwords (based on NLTK `stopwords`). An empty tweet (after preprocessing) and its country label should be **excluded** from the output (`x_processed` and `y_processed`).\n",
    "\n",
    "**Task**: Complete the `preprocess_data(data, labels)` function. The function takes **a list of tweets** and **a corresponding list of country labels** as input, and returns **two lists**. For the first list, each element is a bag-of-words representation of a tweet. For the second list, each element is a corresponding country label. Note that while we do not need to preprocess the country labels (`y`), we need to have a new output list (`y_processed`) because some tweets maybe removed after the preprocessing (due to having an empty set of bag-of-words).\n",
    "\n",
    "**Check**: Use the assertion statements in <b>\"For your testing\"</b> below for the expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of preprocessed tweets = 943\n",
      "Number of preprocessed labels = 943\n",
      "\n",
      "Samples of preprocessed data:\n",
      "Country = us \tTweet = {'@addictd2success': 1, 'thx': 1, 'u': 1, 'following': 1}\n",
      "Country = us \tTweet = {\"let's\": 1, 'say': 1, 'ever': 1, 'switch': 1, 'teams': 1, 'khalesi': 1, 'would': 1, 'top': 1, 'list': 1, '#girlcrush': 1}\n",
      "Country = ph \tTweet = {'taemin': 1, 'jonghyun': 1, 'birits': 1, 'make': 1, 'go': 1, 'http://t.co/le8z3dntla': 1}\n",
      "Country = id \tTweet = {'depart.senior': 1, 'rapat': 1, 'perdana': 1, 'nyayu': 1, 'anita': 1, 'others': 1, 'ruang': 1, 'aescullap': 1, 'fk': 1, 'unsri': 1, 'madang': 1, 'https://t.co/swrallnkrq': 1}\n",
      "Country = ph \tTweet = {'done': 1, 'internship': 1, 'pretty': 1, 'little': 1, 'lady': 1, 'metropolitan': 1, 'medical': 1, 'center': 1, 'w': 1, 'others': 1, 'pic': 1, 'http://t.co/1qh61r1t5r': 1}\n",
      "Country = gb \tTweet = {'wow': 1, \"boruc's\": 1, 'clanger': 1, 'haha': 1, 'sunday': 1, 'league': 1, 'stuff': 1, 'giroud': 1, 'believe': 1, 'luck': 1, '#clown': 1}\n",
      "Country = my \tTweet = {\"i'm\": 1, 'sushi': 1, 'zanmai': 1, 'petaling': 1, 'jaya': 1, 'selangor': 1, 'w': 1, 'others': 1, 'http://t.co/bcnobykz': 1}\n",
      "Country = us \tTweet = {'mega': 1, 'fest': 1, 'going': 1, '@bishopjakes': 1}\n",
      "Country = gb \tTweet = {'@ellexxxpharrell': 1, 'wow': 1, 'love': 1, 'pic': 1, 'babe': 1, 'xx': 1}\n",
      "Country = us \tTweet = {'clue': 1, 'much': 1, 'hurt': 1}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tt = TweetTokenizer()\n",
    "stopwords = set(stopwords.words('english')) #note: stopwords are all in lowercase\n",
    "\n",
    "def preprocess_data(data, labels):\n",
    "    \n",
    "    \n",
    "    ###\n",
    "    # Your answer BEGINS HERE\n",
    "    ###\n",
    "    import re\n",
    "\n",
    "        \n",
    "    x_processed = []\n",
    "    y_processed = []\n",
    "    \n",
    "    # Regex to remove any word that does not contain any English alphabets\n",
    "    r = re.compile('.*[a-z].*')\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        tweet = data[i]\n",
    "        \n",
    "        ## tokenize each tweet into word tokents \n",
    "        tokens = tt.tokenize(tweet)\n",
    "        \n",
    "        # lowercase all words\n",
    "        tokens_lower = [x.lower() for x in tokens]\n",
    "        \n",
    "        # remove any words that do not contain English alphabet\n",
    "        tokens_filtered = list(filter(r.match, tokens_lower))\n",
    "        \n",
    "        # remove stopwords\n",
    "        tokens_filtered = [word for word in tokens_filtered if word not in stopwords]\n",
    "        \n",
    "        tweet_bow = {}\n",
    "        \n",
    "        # convert to BOW representation\n",
    "        for token in tokens_filtered:\n",
    "            if token not in tweet_bow.keys():\n",
    "                tweet_bow[token] = 1\n",
    "            else:\n",
    "                tweet_bow[token] += 1\n",
    "        \n",
    "        # do not return empty tweet / country label\n",
    "        if len(tweet_bow) > 0:\n",
    "            x_processed.append(tweet_bow)\n",
    "            y_processed.append(labels[i])\n",
    "    \n",
    "    return (x_processed, y_processed)\n",
    "\n",
    "    ###\n",
    "    # Your answer ENDS HERE\n",
    "    ###\n",
    "\n",
    "x_processed, y_processed = preprocess_data(x, y)\n",
    "\n",
    "print(\"Number of preprocessed tweets =\", len(x_processed))\n",
    "print(\"Number of preprocessed labels =\", len(y_processed))\n",
    "print(\"\\nSamples of preprocessed data:\")\n",
    "for i in range(10):\n",
    "    print(\"Country =\", y_processed[i], \"\\tTweet =\", x_processed[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For your testing**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(x_processed) == len(y_processed))\n",
    "assert(len(x_processed) > 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions**: Hashtags (i.e. topic tags which start with #) pose an interesting tokenisation problem because they often include multiple words written without spaces or capitalization. Run the code below to collect all unique hashtags in the preprocessed data. **No implementation is needed.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hashtags = 425\n",
      "['#100percentpay', '#1stsundayofoctober', '#1yearofalmostisneverenough', '#2011prdctn', '#2015eebritishfilmacademyawards', '#2k16', '#2littlebirds', '#365picture', '#5sosacousticatlanta', '#5sosfam', '#8thannualpubcrawl', '#affsuzukicup', '#aflpowertigers', '#ahimacon14', '#aim20', '#airasia', '#allcity', '#alliswell', '#allwedoiscurls', '#amazing', '#anferneehardaway', '#ariona', '#art', '#arte', '#artwork', '#ashes', '#asian', '#asiangirl', '#askcrawford', '#askherforfback', '#askolly', '#asksteven', '#at', '#australia', '#awesome', '#awesomepict', '#barcelona', '#bart', '#bayofislands', '#beautiful', '#bedimages', '#bell', '#beringmy', '#bettybooppose', '#bff', '#big', '#bigbertha', '#bigbreakfast', '#blackhat', '#blessedmorethanicanimagine', '#blessedsunday', '#blogtourambiente', '#bluemountains', '#bonekachika', '#boomtaob', '#booyaa', '#bored', '#boredom', '#bradersisterhood', '#breaktime', '#breedingground', '#bringithomemy', '#brooksengland', '#burgers', '#butitsalsokindofaphone', '#bye', '#camera', '#canadaelections', '#cbb', '#cbcolympics', '#cctv', '#cdnpoli', '#celebritycrush', '#chargers', '#chocolate', '#ciosummit', '#cleansidewalk', '#clown', '#coffeespoonart', '#colts', '#confused', '#cornell', '#country', '#craftbeer', '#creative', '#crepes', '#cumannanya', '#danny4mayor', '#data', '#date', '#datingsiteforyou', '#dearmind', '#deed', '#delightful', '#dennisrodman', '#design', '#devacurl', '#die', '#difd', '#diner', '#dinner', '#dragoncon', '#dus', '#dynamounlock', '#earrings', '#eeeeeehhh', '#election2015', '#electriccircus2014', '#endomondo', '#engine', '#english', '#europapark', '#excitables', '#fabulous', '#factorycampus', '#fall', '#familydinner', '#ff', '#fire', '#flambees', '#flashback', '#fly', '#focusateneo', '#followher', '#followme', '#foodporn', '#fotograf√≠as', '#fotorus', '#four', '#freaks', '#friday', '#fridaynight', '#fried', '#friends', '#friendshipflow', '#fries', '#frozenyoghurt', '#fun', '#future', '#galaxy', '#getfreetattooaviciipasses', '#girl', '#girlcrush', '#girls', '#givesmehope', '#goingout', '#google', '#graduated', '#grafunkthiremepls', '#grammyfans', '#grandmarnier', '#greenfood', '#grilled', '#gudnytall', '#gunner', '#gym', '#handbuiltbicycle', '#happybirthdaysandarapark', '#happyfriday', '#harimaumalaya', '#hb60', '#hens', '#hippy', '#holiday', '#hollywoodmusicawards', '#homemadegranola', '#hometomama', '#hot', '#hungergames', '#hungry', '#icu', '#ididntchoosethestudentlifeitchoseme', '#iloveyou', '#imsobored', '#imsosore', '#innoretail', '#insanity', '#insightmedia', '#insightmediasingapore', '#instaframeplus', '#instagood', '#instalook', '#isibaya', '#javaboy', '#jed', '#jewelry', '#jo', '#jordaan', '#jrsurfboards', '#justshare', '#kllive', '#ladygaga', '#latepost', '#laugh', '#laundryservice', '#lazysunday', '#learningcommunties', '#lebedeintennis', '#letmesleep', '#lfc', '#lgbt', '#life', '#lipstickfree', '#littlemonsters', '#loadsoffun', '#lol', '#longranger', '#lotsoflove', '#love', '#lovers', '#lovethisgirl', '#lovevibes', '#lte', '#magazinesandtvscreens', '#magic', '#makeupfree', '#makingemuklajawabnya', '#mamajeanneandme', '#mancrush', '#march', '#massacreconspiracy', '#mauce', '#mavic', '#me', '#meetup', '#melbourne', '#michaelkors', '#mindfulness', '#mkmedi', '#mobile', '#morning', '#mountains', '#movies', '#mtlnewtech', '#mtvhottest', '#mushroom', '#music', '#mustfollow', '#mwc14', '#mybabyemilia', '#myfriendsarebetterthanyours', '#nced', '#ncga', '#ncpol', '#nevergetsold', '#new', '#newlooks', '#newrecord', '#nextlevel', '#nfl', '#nickryrie', '#nochillzone', '#nofilter', '#notersnew2014', '#notreally', '#np', '#nye', '#of', '#offtochurch', '#ohyeah', '#oilandgas', '#olah', '#on', '#oops', '#openspace', '#oscars', '#oui', '#palacefansinthemorning', '#panther', '#panthers', '#partyhardpartyy', '#pats', '#pechanga', '#penny', '#peperoni', '#peppermoney', '#photo', '#photoby', '#photography', '#pic', '#pll', '#pmattheashes', '#pop-up', '#positivity', '#potd', '#procrastination', '#promise', '#purplefriday', '#rainorshine', '#reachingyougpeople', '#realgoodbikes', '#redbull', '#retail', '#revfcwh', '#rippaulwalker', '#ritenow', '#rollercoaster', '#rollersmusicawards', '#rollsroyce', '#rose', '#rosegold', '#rt', '#rundude', '#ryanpurrdler', '#sad', '#safm', '#saints', '#samsung', '#sandwich', '#sarahm', '#saulbass', '#scifigeek', '#security', '#seniorbabysenior', '#sexy', '#sfgiants', '#sggirls', '#shakes', '#shakethatbooty', '#shellvpower', '#shenanigans', '#shopaholic', '#siberuang', '#silver', '#singapore', '#singlefighter', '#sinvsmas', '#skeemsaam', '#skullsearcher', '#sl', '#socal', '#sorrynotsorry', '#southampton', '#spafy', '#spicy', '#spider', '#squad', '#stampede2014', '#startupfest', '#startuphub', '#starving', '#statoftheday', '#stayfreshsaturdaydbn', '#stkilda', '#stop', '#stopcomplaining', '#strictlyus', '#studio', '#summer', '#sun', '#sunrise', '#sunshine', '#supremelaundry', '#surfshop', '#swedumtl', '#sycip', '#taintedlove', '#takeabreak', '#tcschleissheim', '#tdwpliveinkl', '#teamcanada', '#tennis', '#thaiexpress', '#thaifood', '#thankful', '#thankyoulord', '#thankyoupatients', '#thecolorrun2014', '#thedisadvantagesofakindle', '#them', '#thenext5goldenyears', '#thevoiceau', '#thewalkingdead', '#thomassabo', '#throwback', '#thursday', '#ticklish', '#toronto', '#tpcoach', '#transit', '#truegrip', '#tuesday', '#tune', '#turbine', '#txlege', '#ujackbastards', '#umg', '#uniexamonasaturday', '#universal', '#uptonogood', '#urbangardening', '#uss', '#usydhereicome', '#usydoweek', '#utopia', '#vanilla', '#vca', '#vegan', '#veganfood', '#vegetables', '#vegetarian', '#video', '#vma', '#voteonedirection', '#vsco', '#vscocam', '#walking', '#watch', '#weare90s', '#wearesocial', '#white', '#wings', '#wok', '#wood', '#work', '#workmates', '#world', '#worldcup2014', '#yellow', '#yiamas', '#ynwa', '#youtube', '#yummy', '#yws13', '#zweihandvollfarm']\n"
     ]
    }
   ],
   "source": [
    "def get_all_hashtags(data):\n",
    "    hashtags = set([])\n",
    "    for d in data:\n",
    "        for word, frequency in d.items():\n",
    "            if word.startswith(\"#\") and len(word) > 1:\n",
    "                hashtags.add(word)\n",
    "    return hashtags\n",
    "\n",
    "hashtags = get_all_hashtags(x_processed)\n",
    "print(\"Number of hashtags =\", len(hashtags))\n",
    "print(sorted(hashtags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 (1.0 mark)\n",
    "\n",
    "**Instructions**: Our task here to tokenize the hashtags, by implementing the **MaxMatch algorithm** discussed in class.\n",
    "\n",
    "NLTK has a list of words that you can use for matching, see starter code below (`words`). Be careful about efficiency with respect to doing word lookups. One extra challenge you have to deal with is that the provided list of words (`words`) includes only lemmas: your MaxMatch algorithm should match inflected forms by converting them into lemmas using the NLTK lemmatizer before matching (provided by the function `lemmatize(word)`). Note that the list of words (`words`) is the only source that you'll use for matching (i.e. you do not need to find  other external word lists). If you are unable to make any longer match, your code should default to matching a single letter.\n",
    "\n",
    "For example, given \"#newrecord\", the algorithm should produce: \\[\"#\", \"new\", \"record\"\\].\n",
    "\n",
    "**Task**: Complete the `tokenize_hashtags(hashtags)` function by implementing the MaxMatch algorithm. The function takes as input **a set of hashtags**, and returns **a dictionary** where key=\"hashtag\" and value=\"a list of tokenised words\".\n",
    "\n",
    "**Check**: Use the assertion statements in <b>\"For your testing\"</b> below for the expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#vanilla ['#', 'vanilla']\n",
      "#vca ['#', 'v', 'ca']\n",
      "#vegan ['#', 'vega', 'n']\n",
      "#veganfood ['#', 'vega', 'n', 'food']\n",
      "#vegetables ['#', 'vegetables']\n",
      "#vegetarian ['#', 'vegetarian']\n",
      "#video ['#', 'video']\n",
      "#vma ['#', 'v', 'ma']\n",
      "#voteonedirection ['#', 'vote', 'one', 'direction']\n",
      "#vsco ['#', 'vs', 'c', 'o']\n",
      "#vscocam ['#', 'vs', 'coca', 'm']\n",
      "#walking ['#', 'walking']\n",
      "#watch ['#', 'watch']\n",
      "#weare90s ['#', 'wear', 'e', '9', '0', 's']\n",
      "#wearesocial ['#', 'weares', 'o', 'c', 'i', 'al']\n",
      "#white ['#', 'white']\n",
      "#wings ['#', 'wings']\n",
      "#wok ['#', 'wo', 'k']\n",
      "#wood ['#', 'wood']\n",
      "#work ['#', 'work']\n",
      "#workmates ['#', 'work', 'mates']\n",
      "#world ['#', 'world']\n",
      "#worldcup2014 ['#', 'world', 'cup', '2', '0', '1', '4']\n",
      "#yellow ['#', 'yellow']\n",
      "#yiamas ['#', 'y', 'i', 'ama', 's']\n",
      "#ynwa ['#', 'yn', 'wa']\n",
      "#youtube ['#', 'you', 'tube']\n",
      "#yummy ['#', 'yummy']\n",
      "#yws13 ['#', 'y', 'ws', '1', '3']\n",
      "#zweihandvollfarm ['#', 'z', 'wei', 'hand', 'vol', 'l', 'farm']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "words = set(nltk.corpus.words.words()) #a list of words provided by NLTK\n",
    "words = set([ word.lower() for word in words ]) #lowercase all the words for better matching\n",
    "\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "\n",
    "\n",
    "def tokenize_hashtags(hashtags):\n",
    "    \n",
    "    ###\n",
    "    # Your answer BEGINS HERE\n",
    "    ###\n",
    "    result = {}\n",
    "    \n",
    "    for hashtag in hashtags:  \n",
    "        tokens = []\n",
    "        \n",
    "        # Copy hashtag\n",
    "        text = hashtag\n",
    "        \n",
    "        position = len(text)  \n",
    "        \n",
    "        while len(text) > 0: \n",
    "            word = lemmatize(text[0:position])\n",
    "            \n",
    "            # if word in corpus or only single letter left\n",
    "            if word in words or len(text[0:position]) == 1:  \n",
    "                # append original version of token\n",
    "                tokens.append(text[0:position]) \n",
    "                text = text[position:]  \n",
    "                position = len(text)\n",
    "            else:  \n",
    "                position = position-1 \n",
    "                \n",
    "        # store tokens for current hashtag\n",
    "        result[hashtag] = tokens\n",
    "        \n",
    "    return result\n",
    "    ###\n",
    "    # Your answer ENDS HERE\n",
    "    ###\n",
    "\n",
    "#tokenise hashtags with MaxMatch\n",
    "tokenized_hashtags = tokenize_hashtags(hashtags)\n",
    "\n",
    "#print results\n",
    "for k, v in sorted(tokenized_hashtags.items())[-30:]:\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For your testing:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(tokenized_hashtags) == len(hashtags))\n",
    "assert(tokenized_hashtags[\"#newrecord\"] == [\"#\", \"new\", \"record\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 (1.0 mark)\n",
    "\n",
    "**Instructions**: Our next task is to tokenize the hashtags again, but this time using a **reversed version of the MaxMatch algorithm**, where matching begins at the end of the hashtag and progresses backwards (e.g. for <i>#helloworld</i>, we would process it right to left, starting from the last character <i>d</i>). Just like before, you should use the provided word list (`words`) for word matching.\n",
    "\n",
    "**Task**: Complete the `tokenize_hashtags_rev(hashtags)` function by the MaxMatch algorithm. The function takes as input **a set of hashtags**, and returns **a dictionary** where key=\"hashtag\" and value=\"a list of tokenised words\".\n",
    "\n",
    "**Check**: Use the assertion statements in <b>\"For your testing\"</b> below for the expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#vanilla ['#', 'vanilla']\n",
      "#vca ['#', 'v', 'ca']\n",
      "#vegan ['#', 'v', 'e', 'gan']\n",
      "#veganfood ['#', 'v', 'e', 'gan', 'food']\n",
      "#vegetables ['#', 'vegetables']\n",
      "#vegetarian ['#', 'vegetarian']\n",
      "#video ['#', 'video']\n",
      "#vma ['#', 'v', 'ma']\n",
      "#voteonedirection ['#', 'vote', 'one', 'direction']\n",
      "#vsco ['#', 'vs', 'c', 'o']\n",
      "#vscocam ['#', 'vs', 'c', 'o', 'cam']\n",
      "#walking ['#', 'walking']\n",
      "#watch ['#', 'watch']\n",
      "#weare90s ['#', 'we', 'are', '9', '0', 's']\n",
      "#wearesocial ['#', 'we', 'are', 'social']\n",
      "#white ['#', 'white']\n",
      "#wings ['#', 'wings']\n",
      "#wok ['#', 'w', 'ok']\n",
      "#wood ['#', 'wood']\n",
      "#work ['#', 'work']\n",
      "#workmates ['#', 'work', 'mates']\n",
      "#world ['#', 'world']\n",
      "#worldcup2014 ['#', 'world', 'cup', '2', '0', '1', '4']\n",
      "#yellow ['#', 'yellow']\n",
      "#yiamas ['#', 'y', 'i', 'a', 'mas']\n",
      "#ynwa ['#', 'yn', 'wa']\n",
      "#youtube ['#', 'you', 'tube']\n",
      "#yummy ['#', 'yummy']\n",
      "#yws13 ['#', 'y', 'ws', '1', '3']\n",
      "#zweihandvollfarm ['#', 'z', 'wei', 'hand', 'vol', 'l', 'farm']\n"
     ]
    }
   ],
   "source": [
    "def tokenize_hashtags_rev(hashtags):\n",
    "    ###\n",
    "    # Your answer BEGINS HERE\n",
    "    ###\n",
    "    result = {}\n",
    "    \n",
    "    for hashtag in hashtags:\n",
    "    \n",
    "        tokens = []\n",
    "        \n",
    "        # Copy hashtag\n",
    "        text = hashtag\n",
    "\n",
    "        position = 0\n",
    "        while len(text) > 0:  \n",
    "            word = lemmatize(text[position:len(text)])  \n",
    "            \n",
    "            # if word in corpus or only single letter left\n",
    "            if word in words or len(text[position:len(text)]) == 1: \n",
    "                tokens.append(text[position:len(text)]) \n",
    "                text = text[0:position]  \n",
    "                position = 0 \n",
    "            else:  \n",
    "                position = position+1  \n",
    "        \n",
    "        # Bring list of tokens in right orer\n",
    "        tokens.reverse()\n",
    "        result[hashtag] = tokens\n",
    "        \n",
    "    return result\n",
    "    ###\n",
    "    # Your answer ENDS HERE\n",
    "    ###\n",
    "\n",
    "    \n",
    "#tokenise hashtags with the reversed version of MaxMatch\n",
    "tokenized_hashtags_rev = tokenize_hashtags_rev(hashtags)\n",
    "\n",
    "#print results\n",
    "for k, v in sorted(tokenized_hashtags_rev.items())[-30:]:\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For your testing:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(tokenized_hashtags_rev) == len(hashtags))\n",
    "assert(tokenized_hashtags_rev[\"#newrecord\"] == [\"#\", \"new\", \"record\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 (1.0 mark)\n",
    "\n",
    "**Instructions**: The two versions of MaxMatch will produce different results for some of the hashtags. For a hastag that has different results, our task here is to use a **unigram language model** (lecture 3) to score them to see which is better. Recall that in a unigram language model we compute P(<i>#</i>, <i>hello</i>, <i>world</i> = P(<i>#</i>)\\*P(<i>hellow</i>)\\*P(<i>world</i>).\n",
    "\n",
    "You should: (1) use the NLTK's Brown corpus (`brown_words`) for collecting word frequencies (note: the words are already tokenised so no further tokenisation is needed); (2) lowercase all words in the corpus; (3) use add-one smoothing when computing the unigram probabilities; and (4) work in the log space to prevent numerical underflow.\n",
    "\n",
    "**Task**: Build a unigram language model with add-one smoothing using the word counts from the Brown corpus. Iterate through the hashtags, and for each hashtag where MaxMatch and reversed MaxMatch produce different results, print the following: (1) the hashtag; (2) the results produced by MaxMatch and reversed MaxMatch; and (3) the log probability of each result as given by the unigram language model. Note: you **do not** need to print the hashtags where MaxMatch and reversed MaxMatch produce the same results.\n",
    "\n",
    "An example output:\n",
    "```\n",
    "1. #abcd\n",
    "MaxMatch = [#, a, bc, d]; LogProb = -2.3\n",
    "Reversed MaxMatch = [#, a, b, cd]; LogProb = -3.5\n",
    "\n",
    "2. #efgh\n",
    "MaxMatch = [#, ef, g, h]; LogProb = -4.2\n",
    "Reversed MaxMatch = [#, e, fgh]; LogProb = -3.1\n",
    "\n",
    "```\n",
    "\n",
    "Have a look at the output, and see if the sequences with better language model scores (i.e. less negative) are generally more coherent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. #1stsundayofoctober\n",
      "MaxMatch = ['#', '1', 'st', 'sunday', 'ofo', 'c', 'tobe', 'r']; LogProb = -92.69\n",
      "Reversed MaxMatch = ['#', '1', 'st', 'sunday', 'of', 'october']; LogProb = -58.69\n",
      "\n",
      "2. #1yearofalmostisneverenough\n",
      "MaxMatch = ['#', '1', 'year', 'of', 'almost', 'is', 'never', 'enough']; LogProb = -60.89\n",
      "Reversed MaxMatch = ['#', '1', 'year', 'of', 'al', 'mos', 'tis', 'never', 'enough']; LogProb = -86.93\n",
      "\n",
      "3. #8thannualpubcrawl\n",
      "MaxMatch = ['#', '8', 'than', 'nu', 'alp', 'u', 'b', 'crawl']; LogProb = -90.10\n",
      "Reversed MaxMatch = ['#', '8', 'th', 'annual', 'pub', 'crawl']; LogProb = -71.65\n",
      "\n",
      "4. #affsuzukicup\n",
      "MaxMatch = ['#', 'a', 'f', 'fs', 'u', 'z', 'u', 'k', 'i', 'cup']; LogProb = -104.56\n",
      "Reversed MaxMatch = ['#', 'a', 'f', 'f', 'suz', 'u', 'k', 'i', 'cup']; LogProb = -91.37\n",
      "\n",
      "5. #ahimacon14\n",
      "MaxMatch = ['#', 'ah', 'ima', 'con', '1', '4']; LogProb = -67.23\n",
      "Reversed MaxMatch = ['#', 'a', 'hi', 'macon', '1', '4']; LogProb = -58.83\n",
      "\n",
      "6. #alliswell\n",
      "MaxMatch = ['#', 'all', 'is', 'well']; LogProb = -32.00\n",
      "Reversed MaxMatch = ['#', 'al', 'li', 'swell']; LogProb = -50.69\n",
      "\n",
      "7. #allwedoiscurls\n",
      "MaxMatch = ['#', 'all', 'wed', 'o', 'is', 'curls']; LogProb = -61.73\n",
      "Reversed MaxMatch = ['#', 'all', 'w', 'edo', 'is', 'curls']; LogProb = -64.33\n",
      "\n",
      "8. #anferneehardaway\n",
      "MaxMatch = ['#', 'an', 'fern', 'e', 'eh', 'ar', 'daw', 'ay']; LogProb = -97.22\n",
      "Reversed MaxMatch = ['#', 'an', 'f', 'er', 'nee', 'hard', 'away']; LogProb = -74.77\n",
      "\n",
      "9. #ariona\n",
      "MaxMatch = ['#', 'arion', 'a']; LogProb = -31.97\n",
      "Reversed MaxMatch = ['#', 'ar', 'i', 'ona']; LogProb = -47.48\n",
      "\n",
      "10. #arte\n",
      "MaxMatch = ['#', 'art', 'e']; LogProb = -32.96\n",
      "Reversed MaxMatch = ['#', 'ar', 'te']; LogProb = -42.02\n",
      "\n",
      "11. #askherforfback\n",
      "MaxMatch = ['#', 'ask', 'her', 'for', 'f', 'back']; LogProb = -51.52\n",
      "Reversed MaxMatch = ['#', 'ask', 'her', 'f', 'orf', 'back']; LogProb = -60.68\n",
      "\n",
      "12. #askolly\n",
      "MaxMatch = ['#', 'ask', 'o', 'l', 'ly']; LogProb = -59.05\n",
      "Reversed MaxMatch = ['#', 'as', 'kol', 'ly']; LogProb = -47.14\n",
      "\n",
      "13. #asksteven\n",
      "MaxMatch = ['#', 'asks', 'te', 'v', 'en']; LogProb = -61.68\n",
      "Reversed MaxMatch = ['#', 'ask', 'steven']; LogProb = -37.16\n",
      "\n",
      "14. #bedimages\n",
      "MaxMatch = ['#', 'bedim', 'ages']; LogProb = -38.07\n",
      "Reversed MaxMatch = ['#', 'bed', 'images']; LogProb = -33.53\n",
      "\n",
      "15. #beringmy\n",
      "MaxMatch = ['#', 'beri', 'n', 'g', 'my']; LogProb = -56.35\n",
      "Reversed MaxMatch = ['#', 'be', 'ring', 'my']; LogProb = -36.21\n",
      "\n",
      "16. #bettybooppose\n",
      "MaxMatch = ['#', 'betty', 'boo', 'p', 'pose']; LogProb = -61.13\n",
      "Reversed MaxMatch = ['#', 'betty', 'bo', 'oppose']; LogProb = -51.18\n",
      "\n",
      "17. #blackhat\n",
      "MaxMatch = ['#', 'black', 'hat']; LogProb = -32.66\n",
      "Reversed MaxMatch = ['#', 'b', 'lac', 'khat']; LogProb = -51.36\n",
      "\n",
      "18. #blessedmorethanicanimagine\n",
      "MaxMatch = ['#', 'blessed', 'more', 'than', 'i', 'can', 'imagine']; LogProb = -60.06\n",
      "Reversed MaxMatch = ['#', 'blessed', 'more', 'th', 'ani', 'can', 'imagine']; LogProb = -75.00\n",
      "\n",
      "19. #blessedsunday\n",
      "MaxMatch = ['#', 'blesseds', 'un', 'day']; LogProb = -46.86\n",
      "Reversed MaxMatch = ['#', 'blessed', 'sunday']; LogProb = -34.76\n",
      "\n",
      "20. #blogtourambiente\n",
      "MaxMatch = ['#', 'blo', 'g', 'tour', 'ambient', 'e']; LogProb = -73.71\n",
      "Reversed MaxMatch = ['#', 'b', 'log', 'tou', 'ram', 'bien', 'te']; LogProb = -89.11\n",
      "\n",
      "21. #booyaa\n",
      "MaxMatch = ['#', 'boo', 'ya', 'a']; LogProb = -43.67\n",
      "Reversed MaxMatch = ['#', 'boo', 'y', 'aa']; LogProb = -52.85\n",
      "\n",
      "22. #bradersisterhood\n",
      "MaxMatch = ['#', 'brad', 'ers', 'ist', 'er', 'hood']; LogProb = -80.58\n",
      "Reversed MaxMatch = ['#', 'brad', 'er', 'sisterhood']; LogProb = -55.33\n",
      "\n",
      "23. #bringithomemy\n",
      "MaxMatch = ['#', 'bring', 'it', 'home', 'my']; LogProb = -42.40\n",
      "Reversed MaxMatch = ['#', 'brin', 'git', 'home', 'my']; LogProb = -54.93\n",
      "\n",
      "24. #brooksengland\n",
      "MaxMatch = ['#', 'brooks', 'en', 'gland']; LogProb = -48.81\n",
      "Reversed MaxMatch = ['#', 'brook', 'sen', 'gland']; LogProb = -52.34\n",
      "\n",
      "25. #burgers\n",
      "MaxMatch = ['#', 'burg', 'ers']; LogProb = -42.02\n",
      "Reversed MaxMatch = ['#', 'bur', 'gers']; LogProb = -42.02\n",
      "\n",
      "26. #butitsalsokindofaphone\n",
      "MaxMatch = ['#', 'but', 'its', 'also', 'kind', 'of', 'a', 'phone']; LogProb = -58.86\n",
      "Reversed MaxMatch = ['#', 'bu', 'tits', 'also', 'kin', 'do', 'fa', 'phone']; LogProb = -91.66\n",
      "\n",
      "27. #cbcolympics\n",
      "MaxMatch = ['#', 'c', 'b', 'coly', 'm', 'pics']; LogProb = -71.74\n",
      "Reversed MaxMatch = ['#', 'c', 'b', 'col', 'ym', 'pics']; LogProb = -74.57\n",
      "\n",
      "28. #cdnpoli\n",
      "MaxMatch = ['#', 'c', 'd', 'n', 'pol', 'i']; LogProb = -63.09\n",
      "Reversed MaxMatch = ['#', 'c', 'd', 'n', 'po', 'li']; LogProb = -70.95\n",
      "\n",
      "29. #ciosummit\n",
      "MaxMatch = ['#', 'c', 'ios', 'um', 'mi', 't']; LogProb = -71.89\n",
      "Reversed MaxMatch = ['#', 'c', 'io', 'summit']; LogProb = -47.97\n",
      "\n",
      "30. #cleansidewalk\n",
      "MaxMatch = ['#', 'cleans', 'ide', 'walk']; LogProb = -50.72\n",
      "Reversed MaxMatch = ['#', 'clean', 'sidewalk']; LogProb = -34.68\n",
      "\n",
      "31. #coffeespoonart\n",
      "MaxMatch = ['#', 'coffees', 'poon', 'art']; LogProb = -50.69\n",
      "Reversed MaxMatch = ['#', 'coffee', 'spoon', 'art']; LogProb = -44.37\n",
      "\n",
      "32. #cornell\n",
      "MaxMatch = ['#', 'cornel', 'l']; LogProb = -39.19\n",
      "Reversed MaxMatch = ['#', 'cor', 'nell']; LogProb = -41.33\n",
      "\n",
      "33. #cumannanya\n",
      "MaxMatch = ['#', 'cum', 'anna', 'n', 'ya']; LogProb = -62.68\n",
      "Reversed MaxMatch = ['#', 'c', 'u', 'mannan', 'ya']; LogProb = -60.98\n",
      "\n",
      "34. #datingsiteforyou\n",
      "MaxMatch = ['#', 'datings', 'it', 'e', 'for', 'you']; LogProb = -53.99\n",
      "Reversed MaxMatch = ['#', 'dating', 'site', 'for', 'you']; LogProb = -47.00\n",
      "\n",
      "35. #difd\n",
      "MaxMatch = ['#', 'di', 'f', 'd']; LogProb = -45.35\n",
      "Reversed MaxMatch = ['#', 'd', 'if', 'd']; LogProb = -40.47\n",
      "\n",
      "36. #endomondo\n",
      "MaxMatch = ['#', 'end', 'om', 'on', 'do']; LogProb = -47.98\n",
      "Reversed MaxMatch = ['#', 'en', 'do', 'mon', 'do']; LogProb = -52.83\n",
      "\n",
      "37. #excitables\n",
      "MaxMatch = ['#', 'excitable', 's']; LogProb = -38.89\n",
      "Reversed MaxMatch = ['#', 'ex', 'c', 'i', 'tables']; LogProb = -51.78\n",
      "\n",
      "38. #flambees\n",
      "MaxMatch = ['#', 'flamb', 'e', 'es']; LogProb = -52.31\n",
      "Reversed MaxMatch = ['#', 'flam', 'bees']; LogProb = -39.25\n",
      "\n",
      "39. #focusateneo\n",
      "MaxMatch = ['#', 'focus', 'aten', 'e', 'o']; LogProb = -59.31\n",
      "Reversed MaxMatch = ['#', 'fo', 'c', 'u', 'sate', 'neo']; LogProb = -76.60\n",
      "\n",
      "40. #foodporn\n",
      "MaxMatch = ['#', 'food', 'po', 'r', 'n']; LogProb = -57.02\n",
      "Reversed MaxMatch = ['#', 'food', 'p', 'or', 'n']; LogProb = -48.69\n",
      "\n",
      "41. #fotograf√≠as\n",
      "MaxMatch = ['#', 'fot', 'og', 'ra', 'f', '√≠', 'as']; LogProb = -84.86\n",
      "Reversed MaxMatch = ['#', 'f', 'oto', 'gra', 'f', '√≠', 'as']; LogProb = -81.94\n",
      "\n",
      "42. #fotorus\n",
      "MaxMatch = ['#', 'fot', 'or', 'us']; LogProb = -41.17\n",
      "Reversed MaxMatch = ['#', 'fo', 'torus']; LogProb = -42.02\n",
      "\n",
      "43. #getfreetattooaviciipasses\n",
      "MaxMatch = ['#', 'get', 'freet', 'at', 'too', 'a', 'vic', 'i', 'i', 'passes']; LogProb = -86.04\n",
      "Reversed MaxMatch = ['#', 'get', 'free', 'tattoo', 'a', 'vic', 'i', 'i', 'passes']; LogProb = -81.79\n",
      "\n",
      "44. #goingout\n",
      "MaxMatch = ['#', 'going', 'out']; LogProb = -28.38\n",
      "Reversed MaxMatch = ['#', 'go', 'in', 'gout']; LogProb = -38.52\n",
      "\n",
      "45. #google\n",
      "MaxMatch = ['#', 'goo', 'g', 'l', 'e']; LogProb = -60.65\n",
      "Reversed MaxMatch = ['#', 'go', 'ogle']; LogProb = -35.58\n",
      "\n",
      "46. #grammyfans\n",
      "MaxMatch = ['#', 'gram', 'my', 'fans']; LogProb = -43.35\n",
      "Reversed MaxMatch = ['#', 'g', 'rammy', 'fans']; LogProb = -50.10\n",
      "\n",
      "47. #grandmarnier\n",
      "MaxMatch = ['#', 'grandma', 'r', 'ni', 'er']; LogProb = -63.73\n",
      "Reversed MaxMatch = ['#', 'grand', 'm', 'arni', 'er']; LogProb = -63.31\n",
      "\n",
      "48. #happybirthdaysandarapark\n",
      "MaxMatch = ['#', 'happy', 'birthdays', 'anda', 'rap', 'ark']; LogProb = -78.35\n",
      "Reversed MaxMatch = ['#', 'happy', 'birthday', 'sand', 'ara', 'park']; LogProb = -68.58\n",
      "\n",
      "49. #harimaumalaya\n",
      "MaxMatch = ['#', 'ha', 'rima', 'um', 'ala', 'ya']; LogProb = -79.72\n",
      "Reversed MaxMatch = ['#', 'h', 'ar', 'i', 'mau', 'mala', 'ya']; LogProb = -84.71\n",
      "\n",
      "50. #hometomama\n",
      "MaxMatch = ['#', 'home', 'toma', 'ma']; LogProb = -46.73\n",
      "Reversed MaxMatch = ['#', 'home', 'tom', 'ama']; LogProb = -44.87\n",
      "\n",
      "51. #ididntchoosethestudentlifeitchoseme\n",
      "MaxMatch = ['#', 'id', 'id', 'n', 'tch', 'o', 'ose', 'the', 'student', 'life', 'itch', 'ose', 'me']; LogProb = -143.65\n",
      "Reversed MaxMatch = ['#', 'i', 'didnt', 'choose', 'the', 'student', 'life', 'it', 'cho', 'seme']; LogProb = -95.90\n",
      "\n",
      "52. #imsobored\n",
      "MaxMatch = ['#', 'i', 'ms', 'o', 'bored']; LogProb = -54.38\n",
      "Reversed MaxMatch = ['#', 'i', 'm', 'so', 'bored']; LogProb = -48.35\n",
      "\n",
      "53. #imsosore\n",
      "MaxMatch = ['#', 'i', 'ms', 'os', 'ore']; LogProb = -59.69\n",
      "Reversed MaxMatch = ['#', 'i', 'm', 'so', 'sore']; LogProb = -48.66\n",
      "\n",
      "54. #innoretail\n",
      "MaxMatch = ['#', 'inn', 'ore', 'tail']; LogProb = -50.51\n",
      "Reversed MaxMatch = ['#', 'in', 'no', 'retail']; LogProb = -35.35\n",
      "\n",
      "55. #insightmediasingapore\n",
      "MaxMatch = ['#', 'insight', 'media', 'sing', 'a', 'pore']; LogProb = -63.56\n",
      "Reversed MaxMatch = ['#', 'insight', 'me', 'di', 'as', 'inga', 'pore']; LogProb = -74.71\n",
      "\n",
      "56. #instagood\n",
      "MaxMatch = ['#', 'ins', 'tag', 'o', 'od']; LogProb = -64.95\n",
      "Reversed MaxMatch = ['#', 'ins', 'ta', 'good']; LogProb = -49.33\n",
      "\n",
      "57. #instalook\n",
      "MaxMatch = ['#', 'ins', 'tal', 'o', 'ok']; LogProb = -64.95\n",
      "Reversed MaxMatch = ['#', 'ins', 'ta', 'look']; LogProb = -50.04\n",
      "\n",
      "58. #isibaya\n",
      "MaxMatch = ['#', 'is', 'iba', 'ya']; LogProb = -45.20\n",
      "Reversed MaxMatch = ['#', 'i', 'si', 'baya']; LogProb = -46.79\n",
      "\n",
      "59. #jordaan\n",
      "MaxMatch = ['#', 'jo', 'r', 'da', 'an']; LogProb = -53.43\n",
      "Reversed MaxMatch = ['#', 'j', 'or', 'da', 'an']; LogProb = -48.74\n",
      "\n",
      "60. #jrsurfboards\n",
      "MaxMatch = ['#', 'j', 'rs', 'urf', 'boards']; LogProb = -64.22\n",
      "Reversed MaxMatch = ['#', 'j', 'r', 'surfboards']; LogProb = -50.42\n",
      "\n",
      "61. #ladygaga\n",
      "MaxMatch = ['#', 'lady', 'gag', 'a']; LogProb = -39.97\n",
      "Reversed MaxMatch = ['#', 'lady', 'g', 'aga']; LogProb = -48.80\n",
      "\n",
      "62. #laundryservice\n",
      "MaxMatch = ['#', 'laundrys', 'er', 'vice']; LogProb = -52.29\n",
      "Reversed MaxMatch = ['#', 'laundry', 'service']; LogProb = -34.47\n",
      "\n",
      "63. #learningcommunties\n",
      "MaxMatch = ['#', 'learning', 'c', 'om', 'munt', 'ies']; LogProb = -75.13\n",
      "Reversed MaxMatch = ['#', 'learning', 'c', 'om', 'm', 'unties']; LogProb = -72.29\n",
      "\n",
      "64. #lebedeintennis\n",
      "MaxMatch = ['#', 'l', 'e', 'bed', 'e', 'in', 'tennis']; LogProb = -70.20\n",
      "Reversed MaxMatch = ['#', 'l', 'e', 'be', 'de', 'in', 'tennis']; LogProb = -65.19\n",
      "\n",
      "65. #letmesleep\n",
      "MaxMatch = ['#', 'let', 'mes', 'leep']; LogProb = -50.07\n",
      "Reversed MaxMatch = ['#', 'let', 'me', 'sleep']; LogProb = -38.81\n",
      "\n",
      "66. #loadsoffun\n",
      "MaxMatch = ['#', 'loads', 'off', 'un']; LogProb = -44.53\n",
      "Reversed MaxMatch = ['#', 'loads', 'of', 'fun']; LogProb = -39.32\n",
      "\n",
      "67. #longranger\n",
      "MaxMatch = ['#', 'long', 'ranger']; LogProb = -34.30\n",
      "Reversed MaxMatch = ['#', 'l', 'on', 'granger']; LogProb = -44.38\n",
      "\n",
      "68. #magazinesandtvscreens\n",
      "MaxMatch = ['#', 'magazines', 'and', 't', 'vs', 'creen', 's']; LogProb = -75.42\n",
      "Reversed MaxMatch = ['#', 'magazine', 'sand', 't', 'v', 'screens']; LogProb = -66.90\n",
      "\n",
      "69. #makeupfree\n",
      "MaxMatch = ['#', 'make', 'up', 'free']; LogProb = -36.24\n",
      "Reversed MaxMatch = ['#', 'ma', 'keup', 'free']; LogProb = -47.47\n",
      "\n",
      "70. #mamajeanneandme\n",
      "MaxMatch = ['#', 'mam', 'a', 'jeanne', 'and', 'me']; LogProb = -56.65\n",
      "Reversed MaxMatch = ['#', 'm', 'ama', 'jeanne', 'and', 'me']; LogProb = -63.17\n",
      "\n",
      "71. #meetup\n",
      "MaxMatch = ['#', 'meet', 'up']; LogProb = -29.47\n",
      "Reversed MaxMatch = ['#', 'me', 'e', 'tup']; LogProb = -45.24\n",
      "\n",
      "72. #melbourne\n",
      "MaxMatch = ['#', 'mel', 'bourn', 'e']; LogProb = -49.42\n",
      "Reversed MaxMatch = ['#', 'm', 'elb', 'our', 'ne']; LogProb = -58.12\n",
      "\n",
      "73. #mtlnewtech\n",
      "MaxMatch = ['#', 'm', 't', 'l', 'newt', 'e', 'c', 'h']; LogProb = -88.14\n",
      "Reversed MaxMatch = ['#', 'm', 't', 'l', 'new', 'tech']; LogProb = -64.42\n",
      "\n",
      "74. #myfriendsarebetterthanyours\n",
      "MaxMatch = ['#', 'my', 'friends', 'are', 'better', 'than', 'yours']; LogProb = -60.61\n",
      "Reversed MaxMatch = ['#', 'my', 'friend', 'sare', 'better', 'than', 'yours']; LogProb = -69.19\n",
      "\n",
      "75. #nced\n",
      "MaxMatch = ['#', 'n', 'ce', 'd']; LogProb = -48.43\n",
      "Reversed MaxMatch = ['#', 'n', 'c', 'ed']; LogProb = -44.92\n",
      "\n",
      "76. #nevergetsold\n",
      "MaxMatch = ['#', 'never', 'gets', 'old']; LogProb = -38.78\n",
      "Reversed MaxMatch = ['#', 'never', 'get', 'sold']; LogProb = -38.99\n",
      "\n",
      "77. #nickryrie\n",
      "MaxMatch = ['#', 'nick', 'r', 'yr', 'ie']; LogProb = -63.11\n",
      "Reversed MaxMatch = ['#', 'nick', 'r', 'y', 'rie']; LogProb = -61.32\n",
      "\n",
      "78. #oilandgas\n",
      "MaxMatch = ['#', 'oil', 'and', 'gas']; LogProb = -36.59\n",
      "Reversed MaxMatch = ['#', 'o', 'i', 'land', 'gas']; LogProb = -48.20\n",
      "\n",
      "79. #olah\n",
      "MaxMatch = ['#', 'o', 'la', 'h']; LogProb = -45.58\n",
      "Reversed MaxMatch = ['#', 'o', 'l', 'ah']; LogProb = -46.76\n",
      "\n",
      "80. #openspace\n",
      "MaxMatch = ['#', 'opens', 'pace']; LogProb = -35.40\n",
      "Reversed MaxMatch = ['#', 'open', 'space']; LogProb = -31.04\n",
      "\n",
      "81. #palacefansinthemorning\n",
      "MaxMatch = ['#', 'palace', 'fans', 'in', 'them', 'or', 'ning']; LogProb = -65.49\n",
      "Reversed MaxMatch = ['#', 'palace', 'fan', 'sin', 'the', 'morning']; LogProb = -56.93\n",
      "\n",
      "82. #pechanga\n",
      "MaxMatch = ['#', 'pech', 'an', 'ga']; LogProb = -47.11\n",
      "Reversed MaxMatch = ['#', 'p', 'e', 'changa']; LogProb = -47.97\n",
      "\n",
      "83. #peperoni\n",
      "MaxMatch = ['#', 'pep', 'er', 'on', 'i']; LogProb = -52.67\n",
      "Reversed MaxMatch = ['#', 'pep', 'e', 'ro', 'ni']; LogProb = -66.32\n",
      "\n",
      "84. #photoby\n",
      "MaxMatch = ['#', 'photo', 'by']; LogProb = -31.65\n",
      "Reversed MaxMatch = ['#', 'pho', 'toby']; LogProb = -42.02\n",
      "\n",
      "85. #pmattheashes\n",
      "MaxMatch = ['#', 'p', 'matt', 'he', 'ashes']; LogProb = -53.19\n",
      "Reversed MaxMatch = ['#', 'p', 'mat', 'the', 'ashes']; LogProb = -50.80\n",
      "\n",
      "86. #potd\n",
      "MaxMatch = ['#', 'pot', 'd']; LogProb = -34.72\n",
      "Reversed MaxMatch = ['#', 'po', 'td']; LogProb = -41.33\n",
      "\n",
      "87. #rainorshine\n",
      "MaxMatch = ['#', 'rain', 'ors', 'hin', 'e']; LogProb = -62.06\n",
      "Reversed MaxMatch = ['#', 'r', 'ai', 'nor', 'shine']; LogProb = -58.61\n",
      "\n",
      "88. #reachingyougpeople\n",
      "MaxMatch = ['#', 'reaching', 'you', 'g', 'people']; LogProb = -48.58\n",
      "Reversed MaxMatch = ['#', 'reaching', 'yo', 'ug', 'people']; LogProb = -59.51\n",
      "\n",
      "89. #ritenow\n",
      "MaxMatch = ['#', 'rite', 'now']; LogProb = -32.64\n",
      "Reversed MaxMatch = ['#', 'rit', 'enow']; LogProb = -42.02\n",
      "\n",
      "90. #samsung\n",
      "MaxMatch = ['#', 'sams', 'un', 'g']; LogProb = -50.56\n",
      "Reversed MaxMatch = ['#', 'sam', 'sung']; LogProb = -34.69\n",
      "\n",
      "91. #scifigeek\n",
      "MaxMatch = ['#', 's', 'c', 'if', 'i', 'geek']; LogProb = -59.86\n",
      "Reversed MaxMatch = ['#', 's', 'c', 'i', 'fi', 'geek']; LogProb = -67.55\n",
      "\n",
      "92. #seniorbabysenior\n",
      "MaxMatch = ['#', 'senior', 'babys', 'en', 'io', 'r']; LogProb = -74.05\n",
      "Reversed MaxMatch = ['#', 'senior', 'baby', 'senior']; LogProb = -44.77\n",
      "\n",
      "93. #shopaholic\n",
      "MaxMatch = ['#', 'shop', 'aho', 'li', 'c']; LogProb = -61.07\n",
      "Reversed MaxMatch = ['#', 'sho', 'paho', 'li', 'c']; LogProb = -65.23\n",
      "\n",
      "94. #siberuang\n",
      "MaxMatch = ['#', 'sib', 'er', 'uang']; LogProb = -56.03\n",
      "Reversed MaxMatch = ['#', 'si', 'ber', 'uang']; LogProb = -55.33\n",
      "\n",
      "95. #singapore\n",
      "MaxMatch = ['#', 'sing', 'a', 'pore']; LogProb = -41.32\n",
      "Reversed MaxMatch = ['#', 's', 'inga', 'pore']; LogProb = -51.79\n",
      "\n",
      "96. #skeemsaam\n",
      "MaxMatch = ['#', 'skee', 'ms', 'aam']; LogProb = -54.93\n",
      "Reversed MaxMatch = ['#', 's', 'k', 'e', 'ems', 'aam']; LogProb = -74.79\n",
      "\n",
      "97. #skullsearcher\n",
      "MaxMatch = ['#', 'skulls', 'ear', 'che', 'r']; LogProb = -61.18\n",
      "Reversed MaxMatch = ['#', 'skull', 'searcher']; LogProb = -40.63\n",
      "\n",
      "98. #socal\n",
      "MaxMatch = ['#', 'soc', 'al']; LogProb = -38.76\n",
      "Reversed MaxMatch = ['#', 'so', 'cal']; LogProb = -33.73\n",
      "\n",
      "99. #southampton\n",
      "MaxMatch = ['#', 'south', 'am', 'p', 'ton']; LogProb = -52.09\n",
      "Reversed MaxMatch = ['#', 's', 'out', 'ham', 'p', 'ton']; LogProb = -63.28\n",
      "\n",
      "100. #startupfest\n",
      "MaxMatch = ['#', 'start', 'up', 'fest']; LogProb = -43.44\n",
      "Reversed MaxMatch = ['#', 'star', 'tup', 'fest']; LogProb = -52.77\n",
      "\n",
      "101. #startuphub\n",
      "MaxMatch = ['#', 'start', 'up', 'hub']; LogProb = -40.95\n",
      "Reversed MaxMatch = ['#', 'star', 'tup', 'hub']; LogProb = -50.28\n",
      "\n",
      "102. #statoftheday\n",
      "MaxMatch = ['#', 'st', 'at', 'oft', 'he', 'day']; LogProb = -59.06\n",
      "Reversed MaxMatch = ['#', 's', 'tat', 'of', 'the', 'day']; LogProb = -52.02\n",
      "\n",
      "103. #strictlyus\n",
      "MaxMatch = ['#', 'strictly', 'us']; LogProb = -31.98\n",
      "Reversed MaxMatch = ['#', 'strict', 'l', 'yus']; LogProb = -50.71\n",
      "\n",
      "104. #surfshop\n",
      "MaxMatch = ['#', 'surfs', 'hop']; LogProb = -40.92\n",
      "Reversed MaxMatch = ['#', 'surf', 'shop']; LogProb = -37.17\n",
      "\n",
      "105. #swedumtl\n",
      "MaxMatch = ['#', 's', 'wed', 'um', 't', 'l']; LogProb = -71.01\n",
      "Reversed MaxMatch = ['#', 's', 'we', 'dum', 't', 'l']; LogProb = -65.83\n",
      "\n",
      "106. #takeabreak\n",
      "MaxMatch = ['#', 'take', 'ab', 'reak']; LogProb = -48.23\n",
      "Reversed MaxMatch = ['#', 'ta', 'kea', 'break']; LogProb = -51.54\n",
      "\n",
      "107. #thankyoulord\n",
      "MaxMatch = ['#', 'thank', 'youl', 'or', 'd']; LogProb = -54.15\n",
      "Reversed MaxMatch = ['#', 'thank', 'you', 'lord']; LogProb = -39.78\n",
      "\n",
      "108. #thankyoupatients\n",
      "MaxMatch = ['#', 'thank', 'youp', 'ati', 'en', 'ts']; LogProb = -78.35\n",
      "Reversed MaxMatch = ['#', 'thank', 'you', 'patients']; LogProb = -40.71\n",
      "\n",
      "109. #thenext5goldenyears\n",
      "MaxMatch = ['#', 'then', 'ex', 't', '5', 'golden', 'years']; LogProb = -69.78\n",
      "Reversed MaxMatch = ['#', 'the', 'next', '5', 'golden', 'years']; LogProb = -51.33\n",
      "\n",
      "110. #thevoiceau\n",
      "MaxMatch = ['#', 'the', 'voice', 'a', 'u']; LogProb = -40.76\n",
      "Reversed MaxMatch = ['#', 'the', 'v', 'o', 'i', 'c', 'ea', 'u']; LogProb = -78.28\n",
      "\n",
      "111. #thewalkingdead\n",
      "MaxMatch = ['#', 'thew', 'alk', 'ing', 'dead']; LogProb = -64.87\n",
      "Reversed MaxMatch = ['#', 'the', 'walking', 'dead']; LogProb = -35.70\n",
      "\n",
      "112. #toronto\n",
      "MaxMatch = ['#', 'toro', 'n', 'to']; LogProb = -42.19\n",
      "Reversed MaxMatch = ['#', 'tor', 'onto']; LogProb = -37.91\n",
      "\n",
      "113. #txlege\n",
      "MaxMatch = ['#', 't', 'x', 'leg', 'e']; LogProb = -55.11\n",
      "Reversed MaxMatch = ['#', 't', 'x', 'l', 'e', 'ge']; LogProb = -69.27\n",
      "\n",
      "114. #uniexamonasaturday\n",
      "MaxMatch = ['#', 'unie', 'x', 'am', 'ona', 'saturday']; LogProb = -71.58\n",
      "Reversed MaxMatch = ['#', 'u', 'ni', 'ex', 'a', 'mona', 'saturday']; LogProb = -80.04\n",
      "\n",
      "115. #uptonogood\n",
      "MaxMatch = ['#', 'up', 'ton', 'og', 'o', 'od']; LogProb = -70.56\n",
      "Reversed MaxMatch = ['#', 'up', 'to', 'no', 'good']; LogProb = -37.96\n",
      "\n",
      "116. #usydhereicome\n",
      "MaxMatch = ['#', 'us', 'y', 'd', 'here', 'i', 'come']; LogProb = -64.19\n",
      "Reversed MaxMatch = ['#', 'u', 'syd', 'here', 'i', 'come']; LogProb = -59.78\n",
      "\n",
      "117. #usydoweek\n",
      "MaxMatch = ['#', 'us', 'y', 'dow', 'e', 'e', 'k']; LogProb = -78.31\n",
      "Reversed MaxMatch = ['#', 'us', 'y', 'do', 'week']; LogProb = -48.89\n",
      "\n",
      "118. #vegan\n",
      "MaxMatch = ['#', 'vega', 'n']; LogProb = -38.36\n",
      "Reversed MaxMatch = ['#', 'v', 'e', 'gan']; LogProb = -48.98\n",
      "\n",
      "119. #veganfood\n",
      "MaxMatch = ['#', 'vega', 'n', 'food']; LogProb = -47.37\n",
      "Reversed MaxMatch = ['#', 'v', 'e', 'gan', 'food']; LogProb = -57.99\n",
      "\n",
      "120. #vscocam\n",
      "MaxMatch = ['#', 'vs', 'coca', 'm']; LogProb = -51.59\n",
      "Reversed MaxMatch = ['#', 'vs', 'c', 'o', 'cam']; LogProb = -59.63\n",
      "\n",
      "121. #weare90s\n",
      "MaxMatch = ['#', 'wear', 'e', '9', '0', 's']; LogProb = -66.33\n",
      "Reversed MaxMatch = ['#', 'we', 'are', '9', '0', 's']; LogProb = -57.36\n",
      "\n",
      "122. #wearesocial\n",
      "MaxMatch = ['#', 'weares', 'o', 'c', 'i', 'al']; LogProb = -64.13\n",
      "Reversed MaxMatch = ['#', 'we', 'are', 'social']; LogProb = -33.81\n",
      "\n",
      "123. #wok\n",
      "MaxMatch = ['#', 'wo', 'k']; LogProb = -39.62\n",
      "Reversed MaxMatch = ['#', 'w', 'ok']; LogProb = -38.44\n",
      "\n",
      "124. #yiamas\n",
      "MaxMatch = ['#', 'y', 'i', 'ama', 's']; LogProb = -55.86\n",
      "Reversed MaxMatch = ['#', 'y', 'i', 'a', 'mas']; LogProb = -49.64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "#words from brown corpus\n",
    "brown_words = brown.words()\n",
    "\n",
    "###\n",
    "# Your answer BEGINS HERE\n",
    "###\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "# lower-case all words\n",
    "brown_words_lower = [word.lower() for word in brown_words]\n",
    "\n",
    "def get_counts(sentences):\n",
    "    unigram_counts = Counter()\n",
    "\n",
    "    # collect initial unigram statistics\n",
    "    for word in sentences: \n",
    "        unigram_counts[word] += 1\n",
    "    \n",
    "    token_count = float(sum(unigram_counts.values()))\n",
    "    return unigram_counts, token_count\n",
    "\n",
    "# Use add-one smoothing (i.e. k=1) and log when computing probability since probability can get very small\n",
    "def get_log_prob_addk(word, unigram_counts, total_counts, k):\n",
    "    word_occurences = unigram_counts[word] + k\n",
    "    total_occurences = total_counts + k*len(unigram_counts)\n",
    "    return math.log(word_occurences / total_occurences)\n",
    "\n",
    "def get_sent_log_prob_addk(hashtag, unigram_counts, token_count, k):\n",
    "    return sum([get_log_prob_addk(token, unigram_counts, token_count, k) for token in hashtag])\n",
    "\n",
    "brown_unigram_counts, brown_token_count = get_counts(brown_words_lower)\n",
    "\n",
    "count_mismatch = 1\n",
    "# sort alphabetically\n",
    "for hashtag in sorted(hashtags):\n",
    "    if tokenized_hashtags[hashtag] != tokenized_hashtags_rev[hashtag]:\n",
    "        \n",
    "        print(str(count_mismatch) + \". \" + hashtag)\n",
    "        prob_maxmatch = get_sent_log_prob_addk(tokenized_hashtags[hashtag], brown_unigram_counts, brown_token_count, k=1)\n",
    "        prob_maxmatch_rev = get_sent_log_prob_addk(tokenized_hashtags_rev[hashtag], brown_unigram_counts, brown_token_count, k=1)\n",
    "\n",
    "\n",
    "        print(\"MaxMatch = %s; LogProb = %.2f\" % (tokenized_hashtags[hashtag], prob_maxmatch))\n",
    "        print(\"Reversed MaxMatch = %s; LogProb = %.2f\" % (tokenized_hashtags_rev[hashtag], prob_maxmatch_rev))\n",
    "        print()\n",
    "        count_mismatch += 1\n",
    "    \n",
    "###\n",
    "# Your answer ENDS HERE\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification (4 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 (1.0 mark)\n",
    "\n",
    "**Instructions**: Here we are interested to do text classification, to predict the country of origin of a given tweet. The task here is to create training, development and test partitions from the preprocessed data (`x_processed`) and convert the bag-of-words representation into feature vectors.\n",
    "\n",
    "**Task**: Create training, development and test partitions with a 70%/15%/15% ratio. Remember to preserve the ratio of the classes for all your partitions. That is, say we have only 2 classes and 70% of instances are labelled class A and 30% of instances are labelled class B, then the instances in training, development and test partitions should also preserve this 7:3 ratio. You may use sklearn's builtin functions for doing data partitioning.\n",
    "\n",
    "Next, turn the bag-of-words dictionary of each tweet into a feature vector. You may also use sklearn's builtin functions for doing this.\n",
    "\n",
    "You should produce 6 objects: `x_train`, `x_dev`, `x_test` which contain the input feature vectors, and `y_train`, `y_dev` and `y_test` which contain the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "x_train, x_dev, x_test = None, None, None\n",
    "y_train, y_dev, y_test = None, None, None\n",
    "\n",
    "###\n",
    "# Your answer BEGINS HERE\n",
    "###\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_ratio = 0.70\n",
    "dev_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "\n",
    "# Class imbalances are handled via stratify\n",
    "\n",
    "# x_train is now 70% of the entire data set\n",
    "x_train, x_dev_and_test, y_train, y_dev_and_test = train_test_split(x_processed, y_processed, test_size=1 - train_ratio, stratify = y_processed)\n",
    "\n",
    "# x_test is now 15% of the entire data set, x_dev is now 15% of the initial data set\n",
    "x_dev, x_test, y_dev, y_test = train_test_split(x_dev_and_test, y_dev_and_test, test_size=test_ratio/(test_ratio + dev_ratio), stratify = y_dev_and_test)\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "x_train = vectorizer.fit_transform(x_train)\n",
    "x_dev = vectorizer.transform(x_dev)\n",
    "x_test = vectorizer.transform(x_test)\n",
    "\n",
    "###\n",
    "# Your answer ENDS HERE\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6 (1.0 mark)\n",
    "\n",
    "**Instructions**: Now, let's build some classifiers. Here, we'll be comparing Naive Bayes and Logistic Regression. For each, you need to first find a good value for their main regularisation hyper-parameters, which you should identify using the scikit-learn docs or other resources. Use the development set you created for this tuning process; do **not** use cross-validation in the training set, or involve the test set in any way. You don't need to show all your work, but you do need to print out the **accuracy** with enough different settings to strongly suggest you have found an optimal or near-optimal choice. We should not need to look at your code to interpret the output.\n",
    "\n",
    "**Task**: Implement two text classifiers: Naive Bayes and Logistic Regression. Tune the hyper-parameters of these classifiers and print the task performance (accuracy) for different hyper-parameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "Naive Bayes - Best Estimator:  MultinomialNB(alpha=0.01)\n",
      "\n",
      "Naive Bayes - Best Score:  0.24822695035460993\n",
      "\n",
      "Naive Bayes - Hyperparameters:  [{'alpha': 0.01}, {'alpha': 0.014994994994994995}, {'alpha': 0.019989989989989992}, {'alpha': 0.024984984984984984}, {'alpha': 0.029979979979979983}, {'alpha': 0.034974974974974975}, {'alpha': 0.03996996996996997}, {'alpha': 0.04496496496496497}, {'alpha': 0.049959959959959964}, {'alpha': 0.054954954954954956}, {'alpha': 0.059949949949949954}, {'alpha': 0.06494494494494495}, {'alpha': 0.06993993993993994}, {'alpha': 0.07493493493493493}, {'alpha': 0.07992992992992994}, {'alpha': 0.08492492492492493}, {'alpha': 0.08991991991991992}, {'alpha': 0.09491491491491491}, {'alpha': 0.0999099099099099}, {'alpha': 0.10490490490490491}, {'alpha': 0.1098998998998999}, {'alpha': 0.11489489489489489}, {'alpha': 0.1198898898898899}, {'alpha': 0.12488488488488489}, {'alpha': 0.1298798798798799}, {'alpha': 0.13487487487487487}, {'alpha': 0.13986986986986988}, {'alpha': 0.14486486486486488}, {'alpha': 0.1498598598598599}, {'alpha': 0.15485485485485487}, {'alpha': 0.15984984984984987}, {'alpha': 0.16484484484484485}, {'alpha': 0.16983983983983986}, {'alpha': 0.17483483483483486}, {'alpha': 0.17982982982982984}, {'alpha': 0.18482482482482485}, {'alpha': 0.18981981981981982}, {'alpha': 0.19481481481481483}, {'alpha': 0.19980980980980984}, {'alpha': 0.2048048048048048}, {'alpha': 0.20979979979979982}, {'alpha': 0.21479479479479482}, {'alpha': 0.2197897897897898}, {'alpha': 0.2247847847847848}, {'alpha': 0.2297797797797798}, {'alpha': 0.2347747747747748}, {'alpha': 0.2397697697697698}, {'alpha': 0.24476476476476478}, {'alpha': 0.24975975975975978}, {'alpha': 0.2547547547547548}, {'alpha': 0.25974974974974974}, {'alpha': 0.26474474474474474}, {'alpha': 0.26973973973973975}, {'alpha': 0.27473473473473475}, {'alpha': 0.27972972972972976}, {'alpha': 0.28472472472472476}, {'alpha': 0.28971971971971977}, {'alpha': 0.2947147147147147}, {'alpha': 0.2997097097097097}, {'alpha': 0.30470470470470473}, {'alpha': 0.30969969969969974}, {'alpha': 0.31469469469469474}, {'alpha': 0.3196896896896897}, {'alpha': 0.3246846846846847}, {'alpha': 0.3296796796796797}, {'alpha': 0.3346746746746747}, {'alpha': 0.3396696696696697}, {'alpha': 0.34466466466466467}, {'alpha': 0.34965965965965967}, {'alpha': 0.3546546546546547}, {'alpha': 0.3596496496496497}, {'alpha': 0.3646446446446447}, {'alpha': 0.36963963963963964}, {'alpha': 0.37463463463463464}, {'alpha': 0.37962962962962965}, {'alpha': 0.38462462462462466}, {'alpha': 0.38961961961961966}, {'alpha': 0.39461461461461467}, {'alpha': 0.3996096096096096}, {'alpha': 0.4046046046046046}, {'alpha': 0.40959959959959963}, {'alpha': 0.41459459459459463}, {'alpha': 0.41958958958958964}, {'alpha': 0.4245845845845846}, {'alpha': 0.4295795795795796}, {'alpha': 0.4345745745745746}, {'alpha': 0.4395695695695696}, {'alpha': 0.4445645645645646}, {'alpha': 0.4495595595595596}, {'alpha': 0.45455455455455457}, {'alpha': 0.4595495495495496}, {'alpha': 0.4645445445445446}, {'alpha': 0.4695395395395396}, {'alpha': 0.4745345345345346}, {'alpha': 0.47952952952952954}, {'alpha': 0.48452452452452455}, {'alpha': 0.48951951951951955}, {'alpha': 0.49451451451451456}, {'alpha': 0.49950950950950956}, {'alpha': 0.5045045045045045}, {'alpha': 0.5094994994994995}, {'alpha': 0.5144944944944946}, {'alpha': 0.5194894894894895}, {'alpha': 0.5244844844844845}, {'alpha': 0.5294794794794795}, {'alpha': 0.5344744744744745}, {'alpha': 0.5394694694694695}, {'alpha': 0.5444644644644645}, {'alpha': 0.5494594594594595}, {'alpha': 0.5544544544544545}, {'alpha': 0.5594494494494495}, {'alpha': 0.5644444444444445}, {'alpha': 0.5694394394394395}, {'alpha': 0.5744344344344344}, {'alpha': 0.5794294294294294}, {'alpha': 0.5844244244244244}, {'alpha': 0.5894194194194194}, {'alpha': 0.5944144144144144}, {'alpha': 0.5994094094094095}, {'alpha': 0.6044044044044045}, {'alpha': 0.6093993993993995}, {'alpha': 0.6143943943943945}, {'alpha': 0.6193893893893895}, {'alpha': 0.6243843843843844}, {'alpha': 0.6293793793793794}, {'alpha': 0.6343743743743744}, {'alpha': 0.6393693693693694}, {'alpha': 0.6443643643643644}, {'alpha': 0.6493593593593594}, {'alpha': 0.6543543543543544}, {'alpha': 0.6593493493493494}, {'alpha': 0.6643443443443444}, {'alpha': 0.6693393393393394}, {'alpha': 0.6743343343343344}, {'alpha': 0.6793293293293293}, {'alpha': 0.6843243243243243}, {'alpha': 0.6893193193193193}, {'alpha': 0.6943143143143143}, {'alpha': 0.6993093093093093}, {'alpha': 0.7043043043043044}, {'alpha': 0.7092992992992994}, {'alpha': 0.7142942942942944}, {'alpha': 0.7192892892892894}, {'alpha': 0.7242842842842844}, {'alpha': 0.7292792792792793}, {'alpha': 0.7342742742742743}, {'alpha': 0.7392692692692693}, {'alpha': 0.7442642642642643}, {'alpha': 0.7492592592592593}, {'alpha': 0.7542542542542543}, {'alpha': 0.7592492492492493}, {'alpha': 0.7642442442442443}, {'alpha': 0.7692392392392393}, {'alpha': 0.7742342342342343}, {'alpha': 0.7792292292292293}, {'alpha': 0.7842242242242242}, {'alpha': 0.7892192192192192}, {'alpha': 0.7942142142142142}, {'alpha': 0.7992092092092092}, {'alpha': 0.8042042042042042}, {'alpha': 0.8091991991991992}, {'alpha': 0.8141941941941943}, {'alpha': 0.8191891891891893}, {'alpha': 0.8241841841841843}, {'alpha': 0.8291791791791793}, {'alpha': 0.8341741741741743}, {'alpha': 0.8391691691691692}, {'alpha': 0.8441641641641642}, {'alpha': 0.8491591591591592}, {'alpha': 0.8541541541541542}, {'alpha': 0.8591491491491492}, {'alpha': 0.8641441441441442}, {'alpha': 0.8691391391391392}, {'alpha': 0.8741341341341342}, {'alpha': 0.8791291291291292}, {'alpha': 0.8841241241241242}, {'alpha': 0.8891191191191192}, {'alpha': 0.8941141141141141}, {'alpha': 0.8991091091091091}, {'alpha': 0.9041041041041041}, {'alpha': 0.9090990990990991}, {'alpha': 0.9140940940940941}, {'alpha': 0.9190890890890892}, {'alpha': 0.9240840840840842}, {'alpha': 0.9290790790790792}, {'alpha': 0.9340740740740742}, {'alpha': 0.9390690690690692}, {'alpha': 0.9440640640640641}, {'alpha': 0.9490590590590591}, {'alpha': 0.9540540540540541}, {'alpha': 0.9590490490490491}, {'alpha': 0.9640440440440441}, {'alpha': 0.9690390390390391}, {'alpha': 0.9740340340340341}, {'alpha': 0.9790290290290291}, {'alpha': 0.9840240240240241}, {'alpha': 0.9890190190190191}, {'alpha': 0.9940140140140141}, {'alpha': 0.999009009009009}, {'alpha': 1.004004004004004}, {'alpha': 1.008998998998999}, {'alpha': 1.0139939939939941}, {'alpha': 1.0189889889889892}, {'alpha': 1.023983983983984}, {'alpha': 1.028978978978979}, {'alpha': 1.033973973973974}, {'alpha': 1.038968968968969}, {'alpha': 1.043963963963964}, {'alpha': 1.048958958958959}, {'alpha': 1.053953953953954}, {'alpha': 1.058948948948949}, {'alpha': 1.063943943943944}, {'alpha': 1.068938938938939}, {'alpha': 1.073933933933934}, {'alpha': 1.078928928928929}, {'alpha': 1.083923923923924}, {'alpha': 1.088918918918919}, {'alpha': 1.093913913913914}, {'alpha': 1.098908908908909}, {'alpha': 1.103903903903904}, {'alpha': 1.108898898898899}, {'alpha': 1.113893893893894}, {'alpha': 1.118888888888889}, {'alpha': 1.123883883883884}, {'alpha': 1.128878878878879}, {'alpha': 1.1338738738738738}, {'alpha': 1.1388688688688688}, {'alpha': 1.1438638638638639}, {'alpha': 1.1488588588588589}, {'alpha': 1.1538538538538539}, {'alpha': 1.1588488488488489}, {'alpha': 1.1638438438438439}, {'alpha': 1.1688388388388389}, {'alpha': 1.1738338338338339}, {'alpha': 1.178828828828829}, {'alpha': 1.183823823823824}, {'alpha': 1.188818818818819}, {'alpha': 1.193813813813814}, {'alpha': 1.198808808808809}, {'alpha': 1.203803803803804}, {'alpha': 1.208798798798799}, {'alpha': 1.213793793793794}, {'alpha': 1.218788788788789}, {'alpha': 1.223783783783784}, {'alpha': 1.228778778778779}, {'alpha': 1.233773773773774}, {'alpha': 1.2387687687687687}, {'alpha': 1.2437637637637637}, {'alpha': 1.2487587587587587}, {'alpha': 1.2537537537537538}, {'alpha': 1.2587487487487488}, {'alpha': 1.2637437437437438}, {'alpha': 1.2687387387387388}, {'alpha': 1.2737337337337338}, {'alpha': 1.2787287287287288}, {'alpha': 1.2837237237237238}, {'alpha': 1.2887187187187188}, {'alpha': 1.2937137137137138}, {'alpha': 1.2987087087087088}, {'alpha': 1.3037037037037038}, {'alpha': 1.3086986986986988}, {'alpha': 1.3136936936936938}, {'alpha': 1.3186886886886888}, {'alpha': 1.3236836836836838}, {'alpha': 1.3286786786786788}, {'alpha': 1.3336736736736738}, {'alpha': 1.3386686686686688}, {'alpha': 1.3436636636636636}, {'alpha': 1.3486586586586586}, {'alpha': 1.3536536536536536}, {'alpha': 1.3586486486486486}, {'alpha': 1.3636436436436437}, {'alpha': 1.3686386386386387}, {'alpha': 1.3736336336336337}, {'alpha': 1.3786286286286287}, {'alpha': 1.3836236236236237}, {'alpha': 1.3886186186186187}, {'alpha': 1.3936136136136137}, {'alpha': 1.3986086086086087}, {'alpha': 1.4036036036036037}, {'alpha': 1.4085985985985987}, {'alpha': 1.4135935935935937}, {'alpha': 1.4185885885885887}, {'alpha': 1.4235835835835837}, {'alpha': 1.4285785785785787}, {'alpha': 1.4335735735735737}, {'alpha': 1.4385685685685687}, {'alpha': 1.4435635635635637}, {'alpha': 1.4485585585585585}, {'alpha': 1.4535535535535535}, {'alpha': 1.4585485485485485}, {'alpha': 1.4635435435435435}, {'alpha': 1.4685385385385386}, {'alpha': 1.4735335335335336}, {'alpha': 1.4785285285285286}, {'alpha': 1.4835235235235236}, {'alpha': 1.4885185185185186}, {'alpha': 1.4935135135135136}, {'alpha': 1.4985085085085086}, {'alpha': 1.5035035035035036}, {'alpha': 1.5084984984984986}, {'alpha': 1.5134934934934936}, {'alpha': 1.5184884884884886}, {'alpha': 1.5234834834834836}, {'alpha': 1.5284784784784786}, {'alpha': 1.5334734734734736}, {'alpha': 1.5384684684684686}, {'alpha': 1.5434634634634636}, {'alpha': 1.5484584584584586}, {'alpha': 1.5534534534534536}, {'alpha': 1.5584484484484484}, {'alpha': 1.5634434434434434}, {'alpha': 1.5684384384384384}, {'alpha': 1.5734334334334334}, {'alpha': 1.5784284284284285}, {'alpha': 1.5834234234234235}, {'alpha': 1.5884184184184185}, {'alpha': 1.5934134134134135}, {'alpha': 1.5984084084084085}, {'alpha': 1.6034034034034035}, {'alpha': 1.6083983983983985}, {'alpha': 1.6133933933933935}, {'alpha': 1.6183883883883885}, {'alpha': 1.6233833833833835}, {'alpha': 1.6283783783783785}, {'alpha': 1.6333733733733735}, {'alpha': 1.6383683683683685}, {'alpha': 1.6433633633633635}, {'alpha': 1.6483583583583585}, {'alpha': 1.6533533533533535}, {'alpha': 1.6583483483483485}, {'alpha': 1.6633433433433433}, {'alpha': 1.6683383383383383}, {'alpha': 1.6733333333333333}, {'alpha': 1.6783283283283283}, {'alpha': 1.6833233233233234}, {'alpha': 1.6883183183183184}, {'alpha': 1.6933133133133134}, {'alpha': 1.6983083083083084}, {'alpha': 1.7033033033033034}, {'alpha': 1.7082982982982984}, {'alpha': 1.7132932932932934}, {'alpha': 1.7182882882882884}, {'alpha': 1.7232832832832834}, {'alpha': 1.7282782782782784}, {'alpha': 1.7332732732732734}, {'alpha': 1.7382682682682684}, {'alpha': 1.7432632632632634}, {'alpha': 1.7482582582582584}, {'alpha': 1.7532532532532534}, {'alpha': 1.7582482482482484}, {'alpha': 1.7632432432432434}, {'alpha': 1.7682382382382384}, {'alpha': 1.7732332332332332}, {'alpha': 1.7782282282282282}, {'alpha': 1.7832232232232232}, {'alpha': 1.7882182182182182}, {'alpha': 1.7932132132132133}, {'alpha': 1.7982082082082083}, {'alpha': 1.8032032032032033}, {'alpha': 1.8081981981981983}, {'alpha': 1.8131931931931933}, {'alpha': 1.8181881881881883}, {'alpha': 1.8231831831831833}, {'alpha': 1.8281781781781783}, {'alpha': 1.8331731731731733}, {'alpha': 1.8381681681681683}, {'alpha': 1.8431631631631633}, {'alpha': 1.8481581581581583}, {'alpha': 1.8531531531531533}, {'alpha': 1.8581481481481483}, {'alpha': 1.8631431431431433}, {'alpha': 1.8681381381381383}, {'alpha': 1.8731331331331333}, {'alpha': 1.8781281281281281}, {'alpha': 1.8831231231231231}, {'alpha': 1.8881181181181181}, {'alpha': 1.8931131131131131}, {'alpha': 1.8981081081081081}, {'alpha': 1.9031031031031032}, {'alpha': 1.9080980980980982}, {'alpha': 1.9130930930930932}, {'alpha': 1.9180880880880882}, {'alpha': 1.9230830830830832}, {'alpha': 1.9280780780780782}, {'alpha': 1.9330730730730732}, {'alpha': 1.9380680680680682}, {'alpha': 1.9430630630630632}, {'alpha': 1.9480580580580582}, {'alpha': 1.9530530530530532}, {'alpha': 1.9580480480480482}, {'alpha': 1.9630430430430432}, {'alpha': 1.9680380380380382}, {'alpha': 1.9730330330330332}, {'alpha': 1.9780280280280282}, {'alpha': 1.983023023023023}, {'alpha': 1.988018018018018}, {'alpha': 1.993013013013013}, {'alpha': 1.998008008008008}, {'alpha': 2.003003003003003}, {'alpha': 2.007997997997998}, {'alpha': 2.012992992992993}, {'alpha': 2.017987987987988}, {'alpha': 2.022982982982983}, {'alpha': 2.027977977977978}, {'alpha': 2.032972972972973}, {'alpha': 2.0379679679679676}, {'alpha': 2.042962962962963}, {'alpha': 2.0479579579579577}, {'alpha': 2.052952952952953}, {'alpha': 2.0579479479479477}, {'alpha': 2.062942942942943}, {'alpha': 2.0679379379379377}, {'alpha': 2.072932932932933}, {'alpha': 2.0779279279279277}, {'alpha': 2.082922922922923}, {'alpha': 2.0879179179179177}, {'alpha': 2.092912912912913}, {'alpha': 2.0979079079079077}, {'alpha': 2.102902902902903}, {'alpha': 2.1078978978978977}, {'alpha': 2.112892892892893}, {'alpha': 2.1178878878878877}, {'alpha': 2.122882882882883}, {'alpha': 2.1278778778778777}, {'alpha': 2.132872872872873}, {'alpha': 2.1378678678678678}, {'alpha': 2.1428628628628625}, {'alpha': 2.1478578578578578}, {'alpha': 2.1528528528528525}, {'alpha': 2.1578478478478478}, {'alpha': 2.1628428428428426}, {'alpha': 2.167837837837838}, {'alpha': 2.1728328328328326}, {'alpha': 2.177827827827828}, {'alpha': 2.1828228228228226}, {'alpha': 2.187817817817818}, {'alpha': 2.1928128128128126}, {'alpha': 2.197807807807808}, {'alpha': 2.2028028028028026}, {'alpha': 2.207797797797798}, {'alpha': 2.2127927927927926}, {'alpha': 2.217787787787788}, {'alpha': 2.2227827827827826}, {'alpha': 2.227777777777778}, {'alpha': 2.2327727727727726}, {'alpha': 2.237767767767768}, {'alpha': 2.2427627627627627}, {'alpha': 2.247757757757758}, {'alpha': 2.2527527527527527}, {'alpha': 2.2577477477477474}, {'alpha': 2.2627427427427427}, {'alpha': 2.2677377377377375}, {'alpha': 2.2727327327327327}, {'alpha': 2.2777277277277275}, {'alpha': 2.2827227227227227}, {'alpha': 2.2877177177177175}, {'alpha': 2.2927127127127127}, {'alpha': 2.2977077077077075}, {'alpha': 2.3027027027027027}, {'alpha': 2.3076976976976975}, {'alpha': 2.3126926926926927}, {'alpha': 2.3176876876876875}, {'alpha': 2.3226826826826827}, {'alpha': 2.3276776776776775}, {'alpha': 2.3326726726726728}, {'alpha': 2.3376676676676675}, {'alpha': 2.3426626626626628}, {'alpha': 2.3476576576576575}, {'alpha': 2.3526526526526528}, {'alpha': 2.3576476476476476}, {'alpha': 2.3626426426426423}, {'alpha': 2.3676376376376376}, {'alpha': 2.3726326326326324}, {'alpha': 2.3776276276276276}, {'alpha': 2.3826226226226224}, {'alpha': 2.3876176176176176}, {'alpha': 2.3926126126126124}, {'alpha': 2.3976076076076076}, {'alpha': 2.4026026026026024}, {'alpha': 2.4075975975975976}, {'alpha': 2.4125925925925924}, {'alpha': 2.4175875875875876}, {'alpha': 2.4225825825825824}, {'alpha': 2.4275775775775776}, {'alpha': 2.4325725725725724}, {'alpha': 2.4375675675675677}, {'alpha': 2.4425625625625624}, {'alpha': 2.4475575575575577}, {'alpha': 2.4525525525525524}, {'alpha': 2.4575475475475477}, {'alpha': 2.4625425425425425}, {'alpha': 2.4675375375375372}, {'alpha': 2.4725325325325325}, {'alpha': 2.4775275275275273}, {'alpha': 2.4825225225225225}, {'alpha': 2.4875175175175173}, {'alpha': 2.4925125125125125}, {'alpha': 2.4975075075075073}, {'alpha': 2.5025025025025025}, {'alpha': 2.5074974974974973}, {'alpha': 2.5124924924924925}, {'alpha': 2.5174874874874873}, {'alpha': 2.5224824824824825}, {'alpha': 2.5274774774774773}, {'alpha': 2.5324724724724725}, {'alpha': 2.5374674674674673}, {'alpha': 2.5424624624624625}, {'alpha': 2.5474574574574573}, {'alpha': 2.5524524524524526}, {'alpha': 2.5574474474474473}, {'alpha': 2.5624424424424426}, {'alpha': 2.5674374374374374}, {'alpha': 2.572432432432432}, {'alpha': 2.5774274274274274}, {'alpha': 2.582422422422422}, {'alpha': 2.5874174174174174}, {'alpha': 2.592412412412412}, {'alpha': 2.5974074074074074}, {'alpha': 2.602402402402402}, {'alpha': 2.6073973973973974}, {'alpha': 2.612392392392392}, {'alpha': 2.6173873873873874}, {'alpha': 2.622382382382382}, {'alpha': 2.6273773773773774}, {'alpha': 2.632372372372372}, {'alpha': 2.6373673673673674}, {'alpha': 2.642362362362362}, {'alpha': 2.6473573573573574}, {'alpha': 2.6523523523523522}, {'alpha': 2.6573473473473475}, {'alpha': 2.6623423423423422}, {'alpha': 2.6673373373373375}, {'alpha': 2.6723323323323322}, {'alpha': 2.677327327327327}, {'alpha': 2.6823223223223223}, {'alpha': 2.687317317317317}, {'alpha': 2.6923123123123123}, {'alpha': 2.697307307307307}, {'alpha': 2.7023023023023023}, {'alpha': 2.707297297297297}, {'alpha': 2.7122922922922923}, {'alpha': 2.717287287287287}, {'alpha': 2.7222822822822823}, {'alpha': 2.727277277277277}, {'alpha': 2.7322722722722723}, {'alpha': 2.737267267267267}, {'alpha': 2.7422622622622623}, {'alpha': 2.747257257257257}, {'alpha': 2.7522522522522523}, {'alpha': 2.757247247247247}, {'alpha': 2.7622422422422424}, {'alpha': 2.767237237237237}, {'alpha': 2.7722322322322324}, {'alpha': 2.777227227227227}, {'alpha': 2.782222222222222}, {'alpha': 2.787217217217217}, {'alpha': 2.792212212212212}, {'alpha': 2.797207207207207}, {'alpha': 2.802202202202202}, {'alpha': 2.807197197197197}, {'alpha': 2.812192192192192}, {'alpha': 2.817187187187187}, {'alpha': 2.822182182182182}, {'alpha': 2.827177177177177}, {'alpha': 2.832172172172172}, {'alpha': 2.837167167167167}, {'alpha': 2.842162162162162}, {'alpha': 2.8471571571571572}, {'alpha': 2.852152152152152}, {'alpha': 2.8571471471471472}, {'alpha': 2.862142142142142}, {'alpha': 2.8671371371371372}, {'alpha': 2.872132132132132}, {'alpha': 2.8771271271271273}, {'alpha': 2.882122122122122}, {'alpha': 2.887117117117117}, {'alpha': 2.892112112112112}, {'alpha': 2.897107107107107}, {'alpha': 2.902102102102102}, {'alpha': 2.907097097097097}, {'alpha': 2.912092092092092}, {'alpha': 2.917087087087087}, {'alpha': 2.922082082082082}, {'alpha': 2.927077077077077}, {'alpha': 2.932072072072072}, {'alpha': 2.937067067067067}, {'alpha': 2.942062062062062}, {'alpha': 2.947057057057057}, {'alpha': 2.952052052052052}, {'alpha': 2.957047047047047}, {'alpha': 2.962042042042042}, {'alpha': 2.967037037037037}, {'alpha': 2.972032032032032}, {'alpha': 2.977027027027027}, {'alpha': 2.982022022022022}, {'alpha': 2.987017017017017}, {'alpha': 2.992012012012012}, {'alpha': 2.997007007007007}, {'alpha': 3.0020020020020017}, {'alpha': 3.006996996996997}, {'alpha': 3.0119919919919917}, {'alpha': 3.016986986986987}, {'alpha': 3.0219819819819818}, {'alpha': 3.026976976976977}, {'alpha': 3.0319719719719718}, {'alpha': 3.036966966966967}, {'alpha': 3.041961961961962}, {'alpha': 3.046956956956957}, {'alpha': 3.051951951951952}, {'alpha': 3.056946946946947}, {'alpha': 3.061941941941942}, {'alpha': 3.066936936936937}, {'alpha': 3.071931931931932}, {'alpha': 3.076926926926927}, {'alpha': 3.081921921921922}, {'alpha': 3.086916916916917}, {'alpha': 3.091911911911912}, {'alpha': 3.096906906906907}, {'alpha': 3.101901901901902}, {'alpha': 3.1068968968968966}, {'alpha': 3.111891891891892}, {'alpha': 3.1168868868868866}, {'alpha': 3.121881881881882}, {'alpha': 3.1268768768768767}, {'alpha': 3.131871871871872}, {'alpha': 3.1368668668668667}, {'alpha': 3.141861861861862}, {'alpha': 3.1468568568568567}, {'alpha': 3.151851851851852}, {'alpha': 3.1568468468468467}, {'alpha': 3.161841841841842}, {'alpha': 3.1668368368368367}, {'alpha': 3.171831831831832}, {'alpha': 3.1768268268268267}, {'alpha': 3.181821821821822}, {'alpha': 3.1868168168168167}, {'alpha': 3.191811811811812}, {'alpha': 3.1968068068068067}, {'alpha': 3.201801801801802}, {'alpha': 3.2067967967967967}, {'alpha': 3.2117917917917915}, {'alpha': 3.2167867867867868}, {'alpha': 3.2217817817817815}, {'alpha': 3.2267767767767768}, {'alpha': 3.2317717717717716}, {'alpha': 3.236766766766767}, {'alpha': 3.2417617617617616}, {'alpha': 3.246756756756757}, {'alpha': 3.2517517517517516}, {'alpha': 3.256746746746747}, {'alpha': 3.2617417417417416}, {'alpha': 3.266736736736737}, {'alpha': 3.2717317317317316}, {'alpha': 3.276726726726727}, {'alpha': 3.2817217217217216}, {'alpha': 3.286716716716717}, {'alpha': 3.2917117117117116}, {'alpha': 3.296706706706707}, {'alpha': 3.3017017017017016}, {'alpha': 3.306696696696697}, {'alpha': 3.3116916916916916}, {'alpha': 3.3166866866866864}, {'alpha': 3.3216816816816817}, {'alpha': 3.3266766766766764}, {'alpha': 3.3316716716716717}, {'alpha': 3.3366666666666664}, {'alpha': 3.3416616616616617}, {'alpha': 3.3466566566566565}, {'alpha': 3.3516516516516517}, {'alpha': 3.3566466466466465}, {'alpha': 3.3616416416416417}, {'alpha': 3.3666366366366365}, {'alpha': 3.3716316316316317}, {'alpha': 3.3766266266266265}, {'alpha': 3.3816216216216217}, {'alpha': 3.3866166166166165}, {'alpha': 3.3916116116116117}, {'alpha': 3.3966066066066065}, {'alpha': 3.4016016016016017}, {'alpha': 3.4065965965965965}, {'alpha': 3.4115915915915918}, {'alpha': 3.4165865865865865}, {'alpha': 3.4215815815815813}, {'alpha': 3.4265765765765765}, {'alpha': 3.4315715715715713}, {'alpha': 3.4365665665665666}, {'alpha': 3.4415615615615613}, {'alpha': 3.4465565565565566}, {'alpha': 3.4515515515515514}, {'alpha': 3.4565465465465466}, {'alpha': 3.4615415415415414}, {'alpha': 3.4665365365365366}, {'alpha': 3.4715315315315314}, {'alpha': 3.4765265265265266}, {'alpha': 3.4815215215215214}, {'alpha': 3.4865165165165166}, {'alpha': 3.4915115115115114}, {'alpha': 3.4965065065065066}, {'alpha': 3.5015015015015014}, {'alpha': 3.5064964964964966}, {'alpha': 3.5114914914914914}, {'alpha': 3.5164864864864867}, {'alpha': 3.5214814814814814}, {'alpha': 3.5264764764764767}, {'alpha': 3.5314714714714714}, {'alpha': 3.5364664664664662}, {'alpha': 3.5414614614614615}, {'alpha': 3.5464564564564562}, {'alpha': 3.5514514514514515}, {'alpha': 3.5564464464464463}, {'alpha': 3.5614414414414415}, {'alpha': 3.5664364364364363}, {'alpha': 3.5714314314314315}, {'alpha': 3.5764264264264263}, {'alpha': 3.5814214214214215}, {'alpha': 3.5864164164164163}, {'alpha': 3.5914114114114115}, {'alpha': 3.5964064064064063}, {'alpha': 3.6014014014014015}, {'alpha': 3.6063963963963963}, {'alpha': 3.6113913913913915}, {'alpha': 3.6163863863863863}, {'alpha': 3.6213813813813815}, {'alpha': 3.6263763763763763}, {'alpha': 3.6313713713713716}, {'alpha': 3.6363663663663663}, {'alpha': 3.641361361361361}, {'alpha': 3.6463563563563564}, {'alpha': 3.651351351351351}, {'alpha': 3.6563463463463464}, {'alpha': 3.661341341341341}, {'alpha': 3.6663363363363364}, {'alpha': 3.671331331331331}, {'alpha': 3.6763263263263264}, {'alpha': 3.681321321321321}, {'alpha': 3.6863163163163164}, {'alpha': 3.691311311311311}, {'alpha': 3.6963063063063064}, {'alpha': 3.701301301301301}, {'alpha': 3.7062962962962964}, {'alpha': 3.711291291291291}, {'alpha': 3.7162862862862864}, {'alpha': 3.721281281281281}, {'alpha': 3.7262762762762764}, {'alpha': 3.7312712712712712}, {'alpha': 3.7362662662662665}, {'alpha': 3.7412612612612612}, {'alpha': 3.746256256256256}, {'alpha': 3.7512512512512513}, {'alpha': 3.756246246246246}, {'alpha': 3.7612412412412413}, {'alpha': 3.766236236236236}, {'alpha': 3.7712312312312313}, {'alpha': 3.776226226226226}, {'alpha': 3.7812212212212213}, {'alpha': 3.786216216216216}, {'alpha': 3.7912112112112113}, {'alpha': 3.796206206206206}, {'alpha': 3.8012012012012013}, {'alpha': 3.806196196196196}, {'alpha': 3.8111911911911913}, {'alpha': 3.816186186186186}, {'alpha': 3.8211811811811813}, {'alpha': 3.826176176176176}, {'alpha': 3.8311711711711713}, {'alpha': 3.836166166166166}, {'alpha': 3.8411611611611614}, {'alpha': 3.846156156156156}, {'alpha': 3.851151151151151}, {'alpha': 3.856146146146146}, {'alpha': 3.861141141141141}, {'alpha': 3.866136136136136}, {'alpha': 3.871131131131131}, {'alpha': 3.876126126126126}, {'alpha': 3.881121121121121}, {'alpha': 3.886116116116116}, {'alpha': 3.891111111111111}, {'alpha': 3.896106106106106}, {'alpha': 3.901101101101101}, {'alpha': 3.906096096096096}, {'alpha': 3.911091091091091}, {'alpha': 3.916086086086086}, {'alpha': 3.921081081081081}, {'alpha': 3.9260760760760762}, {'alpha': 3.931071071071071}, {'alpha': 3.9360660660660662}, {'alpha': 3.941061061061061}, {'alpha': 3.9460560560560562}, {'alpha': 3.951051051051051}, {'alpha': 3.956046046046046}, {'alpha': 3.961041041041041}, {'alpha': 3.966036036036036}, {'alpha': 3.971031031031031}, {'alpha': 3.976026026026026}, {'alpha': 3.981021021021021}, {'alpha': 3.986016016016016}, {'alpha': 3.991011011011011}, {'alpha': 3.996006006006006}, {'alpha': 4.0010010010010015}, {'alpha': 4.005995995995996}, {'alpha': 4.010990990990991}, {'alpha': 4.015985985985986}, {'alpha': 4.020980980980981}, {'alpha': 4.025975975975976}, {'alpha': 4.030970970970971}, {'alpha': 4.035965965965966}, {'alpha': 4.040960960960961}, {'alpha': 4.045955955955956}, {'alpha': 4.050950950950951}, {'alpha': 4.055945945945946}, {'alpha': 4.060940940940941}, {'alpha': 4.0659359359359355}, {'alpha': 4.070930930930931}, {'alpha': 4.075925925925926}, {'alpha': 4.080920920920921}, {'alpha': 4.0859159159159155}, {'alpha': 4.090910910910911}, {'alpha': 4.095905905905906}, {'alpha': 4.100900900900901}, {'alpha': 4.1058958958958955}, {'alpha': 4.110890890890891}, {'alpha': 4.115885885885886}, {'alpha': 4.120880880880881}, {'alpha': 4.125875875875876}, {'alpha': 4.130870870870871}, {'alpha': 4.135865865865866}, {'alpha': 4.140860860860861}, {'alpha': 4.145855855855856}, {'alpha': 4.150850850850851}, {'alpha': 4.155845845845846}, {'alpha': 4.160840840840841}, {'alpha': 4.165835835835836}, {'alpha': 4.17083083083083}, {'alpha': 4.175825825825826}, {'alpha': 4.180820820820821}, {'alpha': 4.185815815815816}, {'alpha': 4.19081081081081}, {'alpha': 4.195805805805806}, {'alpha': 4.200800800800801}, {'alpha': 4.205795795795796}, {'alpha': 4.21079079079079}, {'alpha': 4.215785785785786}, {'alpha': 4.220780780780781}, {'alpha': 4.225775775775776}, {'alpha': 4.2307707707707705}, {'alpha': 4.235765765765766}, {'alpha': 4.240760760760761}, {'alpha': 4.245755755755756}, {'alpha': 4.2507507507507505}, {'alpha': 4.255745745745746}, {'alpha': 4.260740740740741}, {'alpha': 4.265735735735736}, {'alpha': 4.2707307307307305}, {'alpha': 4.275725725725725}, {'alpha': 4.280720720720721}, {'alpha': 4.285715715715716}, {'alpha': 4.2907107107107105}, {'alpha': 4.295705705705705}, {'alpha': 4.300700700700701}, {'alpha': 4.305695695695696}, {'alpha': 4.3106906906906906}, {'alpha': 4.315685685685685}, {'alpha': 4.320680680680681}, {'alpha': 4.325675675675676}, {'alpha': 4.330670670670671}, {'alpha': 4.335665665665665}, {'alpha': 4.340660660660661}, {'alpha': 4.345655655655656}, {'alpha': 4.350650650650651}, {'alpha': 4.355645645645645}, {'alpha': 4.360640640640641}, {'alpha': 4.365635635635636}, {'alpha': 4.370630630630631}, {'alpha': 4.375625625625625}, {'alpha': 4.38062062062062}, {'alpha': 4.385615615615616}, {'alpha': 4.390610610610611}, {'alpha': 4.395605605605605}, {'alpha': 4.4006006006006}, {'alpha': 4.405595595595596}, {'alpha': 4.410590590590591}, {'alpha': 4.4155855855855854}, {'alpha': 4.42058058058058}, {'alpha': 4.425575575575576}, {'alpha': 4.430570570570571}, {'alpha': 4.4355655655655655}, {'alpha': 4.44056056056056}, {'alpha': 4.445555555555556}, {'alpha': 4.450550550550551}, {'alpha': 4.4555455455455455}, {'alpha': 4.46054054054054}, {'alpha': 4.465535535535536}, {'alpha': 4.470530530530531}, {'alpha': 4.4755255255255255}, {'alpha': 4.48052052052052}, {'alpha': 4.485515515515516}, {'alpha': 4.490510510510511}, {'alpha': 4.4955055055055055}, {'alpha': 4.5005005005005}, {'alpha': 4.505495495495495}, {'alpha': 4.510490490490491}, {'alpha': 4.515485485485486}, {'alpha': 4.52048048048048}, {'alpha': 4.525475475475475}, {'alpha': 4.530470470470471}, {'alpha': 4.535465465465466}, {'alpha': 4.54046046046046}, {'alpha': 4.545455455455455}, {'alpha': 4.550450450450451}, {'alpha': 4.555445445445446}, {'alpha': 4.56044044044044}, {'alpha': 4.565435435435435}, {'alpha': 4.570430430430431}, {'alpha': 4.575425425425426}, {'alpha': 4.58042042042042}, {'alpha': 4.585415415415415}, {'alpha': 4.590410410410411}, {'alpha': 4.595405405405406}, {'alpha': 4.6004004004004}, {'alpha': 4.605395395395395}, {'alpha': 4.61039039039039}, {'alpha': 4.615385385385386}, {'alpha': 4.6203803803803805}, {'alpha': 4.625375375375375}, {'alpha': 4.63037037037037}, {'alpha': 4.635365365365366}, {'alpha': 4.6403603603603605}, {'alpha': 4.645355355355355}, {'alpha': 4.65035035035035}, {'alpha': 4.655345345345346}, {'alpha': 4.6603403403403405}, {'alpha': 4.665335335335335}, {'alpha': 4.67033033033033}, {'alpha': 4.675325325325326}, {'alpha': 4.6803203203203205}, {'alpha': 4.685315315315315}, {'alpha': 4.69031031031031}, {'alpha': 4.695305305305306}, {'alpha': 4.7003003003003005}, {'alpha': 4.705295295295295}, {'alpha': 4.71029029029029}, {'alpha': 4.715285285285285}, {'alpha': 4.720280280280281}, {'alpha': 4.725275275275275}, {'alpha': 4.73027027027027}, {'alpha': 4.735265265265265}, {'alpha': 4.740260260260261}, {'alpha': 4.745255255255255}, {'alpha': 4.75025025025025}, {'alpha': 4.755245245245245}, {'alpha': 4.760240240240241}, {'alpha': 4.765235235235235}, {'alpha': 4.77023023023023}, {'alpha': 4.775225225225225}, {'alpha': 4.780220220220221}, {'alpha': 4.785215215215215}, {'alpha': 4.79021021021021}, {'alpha': 4.795205205205205}, {'alpha': 4.800200200200201}, {'alpha': 4.8051951951951954}, {'alpha': 4.81019019019019}, {'alpha': 4.815185185185185}, {'alpha': 4.82018018018018}, {'alpha': 4.8251751751751755}, {'alpha': 4.83017017017017}, {'alpha': 4.835165165165165}, {'alpha': 4.84016016016016}, {'alpha': 4.8451551551551555}, {'alpha': 4.85015015015015}, {'alpha': 4.855145145145145}, {'alpha': 4.86014014014014}, {'alpha': 4.8651351351351355}, {'alpha': 4.87013013013013}, {'alpha': 4.875125125125125}, {'alpha': 4.88012012012012}, {'alpha': 4.8851151151151155}, {'alpha': 4.89011011011011}, {'alpha': 4.895105105105105}, {'alpha': 4.9001001001001}, {'alpha': 4.905095095095096}, {'alpha': 4.91009009009009}, {'alpha': 4.915085085085085}, {'alpha': 4.92008008008008}, {'alpha': 4.925075075075075}, {'alpha': 4.93007007007007}, {'alpha': 4.935065065065065}, {'alpha': 4.94006006006006}, {'alpha': 4.945055055055055}, {'alpha': 4.95005005005005}, {'alpha': 4.955045045045045}, {'alpha': 4.96004004004004}, {'alpha': 4.965035035035035}, {'alpha': 4.97003003003003}, {'alpha': 4.975025025025025}, {'alpha': 4.98002002002002}, {'alpha': 4.985015015015015}, {'alpha': 4.99001001001001}, {'alpha': 4.995005005005005}, {'alpha': 5.0}]\n",
      "\n",
      "Naive Bayes- Task results:  [0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695 0.24822695\n",
      " 0.24822695 0.24822695 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475 0.24113475\n",
      " 0.24113475 0.24113475 0.24113475 0.24113475 0.23404255 0.23404255\n",
      " 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255\n",
      " 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255\n",
      " 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255\n",
      " 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255\n",
      " 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255\n",
      " 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255\n",
      " 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255\n",
      " 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255\n",
      " 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255\n",
      " 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255\n",
      " 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255\n",
      " 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255\n",
      " 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255\n",
      " 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255\n",
      " 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255\n",
      " 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255\n",
      " 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255\n",
      " 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255\n",
      " 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255\n",
      " 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255\n",
      " 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255\n",
      " 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255\n",
      " 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255\n",
      " 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255\n",
      " 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255\n",
      " 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255\n",
      " 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255\n",
      " 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255\n",
      " 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255\n",
      " 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255\n",
      " 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255\n",
      " 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255 0.23404255\n",
      " 0.23404255 0.23404255 0.23404255 0.23404255]\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "Logistic Regression - Best Estimatator:  LogisticRegression(C=3.1191919191919197, max_iter=1000)\n",
      "\n",
      "Logistic Regression - Best Score 0.3049645390070922\n",
      "\n",
      "Logistic Regression - Hyperparameters [{'C': 0.1, 'max_iter': 1000}, {'C': 0.14949494949494951, 'max_iter': 1000}, {'C': 0.198989898989899, 'max_iter': 1000}, {'C': 0.2484848484848485, 'max_iter': 1000}, {'C': 0.297979797979798, 'max_iter': 1000}, {'C': 0.3474747474747475, 'max_iter': 1000}, {'C': 0.396969696969697, 'max_iter': 1000}, {'C': 0.44646464646464656, 'max_iter': 1000}, {'C': 0.49595959595959604, 'max_iter': 1000}, {'C': 0.5454545454545455, 'max_iter': 1000}, {'C': 0.594949494949495, 'max_iter': 1000}, {'C': 0.6444444444444445, 'max_iter': 1000}, {'C': 0.693939393939394, 'max_iter': 1000}, {'C': 0.7434343434343434, 'max_iter': 1000}, {'C': 0.792929292929293, 'max_iter': 1000}, {'C': 0.8424242424242425, 'max_iter': 1000}, {'C': 0.891919191919192, 'max_iter': 1000}, {'C': 0.9414141414141415, 'max_iter': 1000}, {'C': 0.990909090909091, 'max_iter': 1000}, {'C': 1.0404040404040407, 'max_iter': 1000}, {'C': 1.0898989898989901, 'max_iter': 1000}, {'C': 1.1393939393939396, 'max_iter': 1000}, {'C': 1.188888888888889, 'max_iter': 1000}, {'C': 1.2383838383838386, 'max_iter': 1000}, {'C': 1.287878787878788, 'max_iter': 1000}, {'C': 1.3373737373737375, 'max_iter': 1000}, {'C': 1.386868686868687, 'max_iter': 1000}, {'C': 1.4363636363636367, 'max_iter': 1000}, {'C': 1.4858585858585862, 'max_iter': 1000}, {'C': 1.5353535353535357, 'max_iter': 1000}, {'C': 1.5848484848484852, 'max_iter': 1000}, {'C': 1.6343434343434347, 'max_iter': 1000}, {'C': 1.6838383838383841, 'max_iter': 1000}, {'C': 1.7333333333333336, 'max_iter': 1000}, {'C': 1.782828282828283, 'max_iter': 1000}, {'C': 1.8323232323232326, 'max_iter': 1000}, {'C': 1.881818181818182, 'max_iter': 1000}, {'C': 1.9313131313131315, 'max_iter': 1000}, {'C': 1.9808080808080812, 'max_iter': 1000}, {'C': 2.0303030303030307, 'max_iter': 1000}, {'C': 2.07979797979798, 'max_iter': 1000}, {'C': 2.1292929292929297, 'max_iter': 1000}, {'C': 2.178787878787879, 'max_iter': 1000}, {'C': 2.2282828282828286, 'max_iter': 1000}, {'C': 2.277777777777778, 'max_iter': 1000}, {'C': 2.3272727272727276, 'max_iter': 1000}, {'C': 2.376767676767677, 'max_iter': 1000}, {'C': 2.4262626262626266, 'max_iter': 1000}, {'C': 2.475757575757576, 'max_iter': 1000}, {'C': 2.5252525252525255, 'max_iter': 1000}, {'C': 2.574747474747475, 'max_iter': 1000}, {'C': 2.6242424242424245, 'max_iter': 1000}, {'C': 2.673737373737374, 'max_iter': 1000}, {'C': 2.7232323232323234, 'max_iter': 1000}, {'C': 2.7727272727272734, 'max_iter': 1000}, {'C': 2.822222222222223, 'max_iter': 1000}, {'C': 2.8717171717171723, 'max_iter': 1000}, {'C': 2.921212121212122, 'max_iter': 1000}, {'C': 2.9707070707070713, 'max_iter': 1000}, {'C': 3.0202020202020208, 'max_iter': 1000}, {'C': 3.0696969696969703, 'max_iter': 1000}, {'C': 3.1191919191919197, 'max_iter': 1000}, {'C': 3.168686868686869, 'max_iter': 1000}, {'C': 3.2181818181818187, 'max_iter': 1000}, {'C': 3.267676767676768, 'max_iter': 1000}, {'C': 3.3171717171717177, 'max_iter': 1000}, {'C': 3.366666666666667, 'max_iter': 1000}, {'C': 3.4161616161616166, 'max_iter': 1000}, {'C': 3.465656565656566, 'max_iter': 1000}, {'C': 3.5151515151515156, 'max_iter': 1000}, {'C': 3.564646464646465, 'max_iter': 1000}, {'C': 3.6141414141414145, 'max_iter': 1000}, {'C': 3.663636363636364, 'max_iter': 1000}, {'C': 3.7131313131313135, 'max_iter': 1000}, {'C': 3.762626262626263, 'max_iter': 1000}, {'C': 3.812121212121213, 'max_iter': 1000}, {'C': 3.8616161616161624, 'max_iter': 1000}, {'C': 3.911111111111112, 'max_iter': 1000}, {'C': 3.9606060606060614, 'max_iter': 1000}, {'C': 4.01010101010101, 'max_iter': 1000}, {'C': 4.05959595959596, 'max_iter': 1000}, {'C': 4.109090909090909, 'max_iter': 1000}, {'C': 4.158585858585859, 'max_iter': 1000}, {'C': 4.208080808080808, 'max_iter': 1000}, {'C': 4.257575757575758, 'max_iter': 1000}, {'C': 4.307070707070707, 'max_iter': 1000}, {'C': 4.356565656565657, 'max_iter': 1000}, {'C': 4.406060606060606, 'max_iter': 1000}, {'C': 4.455555555555556, 'max_iter': 1000}, {'C': 4.505050505050505, 'max_iter': 1000}, {'C': 4.554545454545455, 'max_iter': 1000}, {'C': 4.604040404040404, 'max_iter': 1000}, {'C': 4.653535353535354, 'max_iter': 1000}, {'C': 4.703030303030303, 'max_iter': 1000}, {'C': 4.752525252525253, 'max_iter': 1000}, {'C': 4.802020202020202, 'max_iter': 1000}, {'C': 4.851515151515152, 'max_iter': 1000}, {'C': 4.901010101010101, 'max_iter': 1000}, {'C': 4.9505050505050505, 'max_iter': 1000}, {'C': 5.0, 'max_iter': 1000}]\n",
      "\n",
      "Logistic Regression - Task results [0.24822695 0.25531915 0.24822695 0.24822695 0.25531915 0.26241135\n",
      " 0.26950355 0.27659574 0.27659574 0.28368794 0.28368794 0.29078014\n",
      " 0.29078014 0.29078014 0.29078014 0.29078014 0.29078014 0.29078014\n",
      " 0.29078014 0.29078014 0.29078014 0.29078014 0.29787234 0.29787234\n",
      " 0.29787234 0.29787234 0.29787234 0.29787234 0.29787234 0.29787234\n",
      " 0.29787234 0.29787234 0.29787234 0.29787234 0.29787234 0.29787234\n",
      " 0.29787234 0.29787234 0.29787234 0.29787234 0.29787234 0.29787234\n",
      " 0.29787234 0.29787234 0.29787234 0.29787234 0.29787234 0.29787234\n",
      " 0.29787234 0.29787234 0.29787234 0.29787234 0.29787234 0.29787234\n",
      " 0.29787234 0.29787234 0.29787234 0.29787234 0.29787234 0.29787234\n",
      " 0.29787234 0.30496454 0.30496454 0.30496454 0.30496454 0.30496454\n",
      " 0.30496454 0.30496454 0.30496454 0.30496454 0.30496454 0.30496454\n",
      " 0.30496454 0.30496454 0.30496454 0.30496454 0.30496454 0.30496454\n",
      " 0.30496454 0.30496454 0.30496454 0.30496454 0.30496454 0.30496454\n",
      " 0.30496454 0.30496454 0.30496454 0.30496454 0.30496454 0.30496454\n",
      " 0.30496454 0.30496454 0.30496454 0.30496454 0.30496454 0.30496454\n",
      " 0.30496454 0.30496454 0.30496454 0.30496454]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "###\n",
    "# Your answer BEGINS HERE\n",
    "###\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import vstack\n",
    "from sklearn.model_selection import PredefinedSplit, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "\n",
    "#Define model\n",
    "NB = MultinomialNB()\n",
    "LR = LogisticRegression()\n",
    "\n",
    "\n",
    "# merge training and validation set for hyperparamer search and fit the split to prevent cross-validation (see below)\n",
    "combined_x_train = vstack((x_train,x_dev))\n",
    "combined_y_train = y_train + y_dev\n",
    "\n",
    "# create array with indexes to x_train / x_dev - shape of array is (801,) initialized with 0.0\n",
    "test_fold = np.zeros(combined_x_train.shape[0])\n",
    "\n",
    "# set the first 660 indices to -1 (this enforces that x_train is used during training phase and not during validation phase) \n",
    "#\n",
    "\n",
    "# \"It is possible to exclude sample i from any test set (i.e. include sample i in every training set) by \n",
    "# setting test_fold[i] equal to -1.\"\n",
    "# Source: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.PredefinedSplit.html\n",
    "\n",
    "test_fold[:x_train.shape[0]] = -1   \n",
    "\n",
    "#Provide indices for train/dev datasets to split data into train/dev sets\n",
    "ps = PredefinedSplit(test_fold=test_fold)  \n",
    "\n",
    "\n",
    "# Grid Search for MultinomialNB\n",
    "NB_grid_params = [\n",
    "  {'alpha': np.linspace(0.01, 5, 1000)}\n",
    " ]\n",
    "\n",
    "# Configure the GridSearch - the PredefinedSplit ensures that no cross-validation is used\n",
    "NB_grid_search = GridSearchCV(NB, NB_grid_params, n_jobs=-1, cv = ps, scoring='accuracy')\n",
    "\n",
    "# Initialize the GridSearch\n",
    "NB_grid_search.fit(combined_x_train, combined_y_train)\n",
    "\n",
    "\n",
    "\n",
    "# GridSearch for LogisticRegression\n",
    "LR_grid_params = [\n",
    "  {'C': np.linspace(0.1, 5, 100), \"max_iter\":[1000] }\n",
    " ]\n",
    "\n",
    "# Configure the GridSearch - the PredefinedSplit ensures that no cross-validation is used\n",
    "LR_grid_search = GridSearchCV(LR, LR_grid_params, n_jobs=-1, cv = ps, scoring='accuracy')\n",
    "\n",
    "# Initialize the GridSearch\n",
    "LR_grid_search.fit(combined_x_train, combined_y_train)\n",
    "\n",
    "\n",
    "print(\"------------------------------------------\")\n",
    "print(\"Naive Bayes - Best Estimator: \",NB_grid_search.best_estimator_)\n",
    "print()\n",
    "print(\"Naive Bayes - Best Score: \",NB_grid_search.best_score_)\n",
    "print()\n",
    "print(\"Naive Bayes - Hyperparameters: \",NB_grid_search.cv_results_[\"params\"])\n",
    "print()\n",
    "print(\"Naive Bayes- Task results: \",NB_grid_search.cv_results_[\"split0_test_score\"])\n",
    "print()\n",
    "print()\n",
    "\n",
    "\n",
    "print(\"------------------------------------------\")\n",
    "print(\"Logistic Regression - Best Estimatator: \",LR_grid_search.best_estimator_)\n",
    "print()\n",
    "print(\"Logistic Regression - Best Score\",LR_grid_search.best_score_)\n",
    "print()\n",
    "print(\"Logistic Regression - Hyperparameters\",LR_grid_search.cv_results_[\"params\"])\n",
    "print()\n",
    "print(\"Logistic Regression - Task results\",LR_grid_search.cv_results_[\"split0_test_score\"])\n",
    "    \n",
    "###\n",
    "# Your answer ENDS HERE\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes - Best Estimator:  MultinomialNB(alpha=0.01)\n",
      "\n",
      "Naive Bayes - Best Score:  0.24822695035460993\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print results from GridSearch\n",
    "import matplotlib.pyplot as plt\n",
    "x = [alpha['alpha']  for alpha in NB_grid_search.cv_results_[\"params\"]]\n",
    "y = [ score for score in NB_grid_search.cv_results_[\"split0_test_score\"] ]\n",
    "plt.figure()\n",
    "plt.plot(x,y)\n",
    "plt.xlabel(r'$ \\alpha - Parameter$')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Naive Bayes Grid Search')\n",
    "\n",
    "print(\"Naive Bayes - Best Estimator: \",NB_grid_search.best_estimator_, )\n",
    "print()\n",
    "print(\"Naive Bayes - Best Score: \",NB_grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Best Estimator:  LogisticRegression(C=3.1191919191919197, max_iter=1000)\n",
      "Logistic Regression - Best Score:  0.3049645390070922\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = [C['C']  for C in LR_grid_search.cv_results_[\"params\"]]\n",
    "y = [ score for score in LR_grid_search.cv_results_[\"split0_test_score\"] ]\n",
    "plt.figure()\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('C - Parameter')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Logistic Regression Grid Search')\n",
    "\n",
    "\n",
    "print(\"Logistic Regression - Best Estimator: \",LR_grid_search.best_estimator_)\n",
    "print(\"Logistic Regression - Best Score: \",LR_grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7 (1.0 mark)\n",
    "\n",
    "**Instructions**: Using the best settings you have found, compare the two classifiers based on performance in the test set. Print out both **accuracy** and **macro-averaged F-score** for each classifier. Be sure to label your output. You may use sklearn's inbuilt functions.\n",
    "\n",
    "**Task**: Compute test performance in terms of accuracy and macro-averaged F-score for both Naive Bayes and Logistic Regression, using their optimal hyper-parameter settings based on their development performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes:\n",
      "Accuracy: 0.345\n",
      "Macro-averaged F1-score: 0.351\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "Logistic Regression:\n",
      "Accuracy : 0.359\n",
      "Macro-averaged F1-score : 0.343\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "# Your answer BEGINS HERE\n",
    "###\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "#Define the model using the best parameters\n",
    "NB_final_model = MultinomialNB(alpha=NB_grid_search.best_params_['alpha'])\n",
    "LR_final_model = LogisticRegression(C=LR_grid_search.best_params_['C'], max_iter=LR_grid_search.best_params_['max_iter'])\n",
    "\n",
    "\n",
    "NB_final_model.fit(x_train, y_train)\n",
    "LR_final_model.fit(x_train, y_train)\n",
    "\n",
    "#Using test sets to predict categories\n",
    "y_pred_NB = NB_final_model.predict(x_test)\n",
    "y_pred_LR = LR_final_model.predict(x_test)\n",
    "\n",
    "#Print the performance on the test sets\n",
    "print(\"Naive Bayes:\")\n",
    "print(\"Accuracy: %.3f\" % (accuracy_score(y_test, y_pred_NB)))\n",
    "print(\"Macro-averaged F1-score: %.3f\" % (f1_score(y_test, y_pred_NB, average='macro')))\n",
    "print()\n",
    "print(\"------------------------------------------\")\n",
    "print()\n",
    "print(\"Logistic Regression:\")\n",
    "print(\"Accuracy : %.3f\" % (accuracy_score(y_test, y_pred_LR)))\n",
    "print(\"Macro-averaged F1-score : %.3f\" %(f1_score(y_test, y_pred_LR, average='macro')))\n",
    "\n",
    "\n",
    "\n",
    "###\n",
    "# Your answer ENDS HERE\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Question 8 (1.0 mark)\n",
    "\n",
    "**Instructions**: Print the most important features and their weights for each class for the two classifiers.\n",
    "\n",
    "\n",
    "**Task**: For each of the classifiers (Logistic Regression and Naive Bayes) you've built in the previous question, print out the top-20 features (words) with the highest weight for each class (countries).\n",
    "\n",
    "An example output:\n",
    "```\n",
    "Classifier = Logistic Regression\n",
    "\n",
    "Country = au\n",
    "aaa (0.999) bbb (0.888) ccc (0.777) ...\n",
    "\n",
    "Country = ca\n",
    "aaa (0.999) bbb (0.888) ccc (0.777) ...\n",
    "\n",
    "Classifier = Naive Bayes\n",
    "\n",
    "Country = au\n",
    "aaa (-1.0) bbb (-2.0) ccc (-3.0) ...\n",
    "\n",
    "Country = ca\n",
    "aaa (-1.0) bbb (-2.0) ccc (-3.0) ...\n",
    "```\n",
    "\n",
    "Have a look at the output, and see if you notice any trend/pattern in the words for each country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier = Logistic Regression\n",
      "\n",
      "Country = au\n",
      "australia: (1.739) #melbourne: (1.659) literally: (1.323) melbourne: (1.307) little: (1.264) brilliant: (1.228) great: (1.135) one: (1.083) #mtvhottest: (1.076) https://t.co/7rcjjptvl7: (1.073) summerpoyi's: (1.073) instagrams: (1.038) spammed: (1.038) hey: (1.025) geelong: (0.926) freo: (0.926) ha: (0.926) @christorrano: (0.926) http://t.co/blhj9cmxit: (0.926) @shell_07: (0.926) \n",
      "\n",
      "Country = ca\n",
      "bed: (1.299) thing: (1.275) really: (1.252) let's: (1.098) kids: (1.047) sounds: (1.038) got: (1.035) @jakkabishop1: (1.001) jacquie: (1.001) movies: (0.958) right: (0.942) found: (0.939) presentation: (0.938) tests: (0.938) @lola9793: (0.933) joking: (0.901) manor: (0.879) carlyle: (0.879) second: (0.866) patty's: (0.846) \n",
      "\n",
      "Country = de\n",
      "love: (1.482) @fabiomarabini: (1.414) happened: (1.414) roseninsel: (1.329) https://t.co/df7ficsci3: (1.329) posted: (1.234) enough: (1.159) https://t.co/brkwmsvzrb: (1.078) gauting: (1.078) #truegrip: (1.078) workout: (1.060) https://t.co/i7j5pmd3mx: (1.060) germany: (1.023) adlib: (1.003) https://t.co/psveyka5g3: (1.003) @selenagomez: (0.970) http://t.co/airtaqn48v: (0.970) night: (0.942) facetime: (0.927) #asksteven: (0.927) \n",
      "\n",
      "Country = gb\n",
      "xx: (1.342) x: (1.331) wow: (1.204) prom: (1.202) this'll: (1.202) interesting: (1.202) http://t.co/w0tjrah9y7: (1.202) plz: (1.116) need: (1.113) chelsea: (1.041) @ryan_hildyard: (1.006) forever: (0.998) http://t.co/tzovimidp9: (0.998) steady: (0.996) @gingersims: (0.996) beautiful: (0.995) @jackclaudereadi: (0.988) http://t.co/giq4dl9jcq: (0.973) modelling: (0.973) liverpool: (0.963) \n",
      "\n",
      "Country = id\n",
      "pic: (1.753) https://t.co/exshmyqmsl: (1.331) http://t.co/0uccot9gdn: (1.300) https://t.co/xxbmsbuf0r: (1.264) selamat: (1.198) @farid_nugrahaa: (1.109) @smandatas_261: (1.109) followback: (1.109) https://t.co/3sxyzl4crq: (1.109) http://t.co/32szhx0tlk: (1.094) cafe: (1.051) rt: (0.993) https://t.co/cxzfcg0hyi: (0.914) earth: (0.914) karaoke: (0.899) @viccent22: (0.883) http://t.co/qciitw5ihf: (0.865) #eeeeeehhh: (0.865) http://t.co/fd2wjjnrus: (0.853) @annisyaharlina: (0.853) \n",
      "\n",
      "Country = my\n",
      "@poemporns: (1.788) @juztakumi: (1.337) thank: (1.217) selangor: (1.126) @aqmarnaimi_: (1.098) hunny: (1.098) news: (1.077) @markthewise: (1.033) depressing: (1.033) http://t.co/6qowzr40be: (1.018) badminton: (0.992) back: (0.985) @nabil_bil_bil: (0.978) wan: (0.978) #dearmind: (0.966) @_amewwa_: (0.942) hekhek: (0.942) thinking: (0.917) http://t.co/utnamcqqx5: (0.912) #gunner: (0.912) \n",
      "\n",
      "Country = ph\n",
      ":d: (1.604) baby: (1.334) @veronicavispo: (1.153) #sorrynotsorry: (1.153) playing: (1.079) reason: (1.075) bath: (1.043) mcdonald's: (1.032) http://t.co/rqesuepr0f: (1.017) subdivision: (0.992) http://t.co/tnbxjbaocu: (0.992) fairlane: (0.992) dye: (0.983) reco: (0.943) http://t.co/h0dlmhdtbf: (0.943) city: (0.935) lol: (0.913) pastries: (0.898) perfection: (0.898) burger: (0.894) \n",
      "\n",
      "Country = sg\n",
      "singapore: (2.463) @ricadetiquez: (1.359) https://t.co/potgoiy5rv: (1.318) @tutiandonlyher: (1.234) use: (1.207) hotel: (1.200) https://t.co/g0022mcf4u: (1.196) https://t.co/bwenaysyrq: (1.196) @sbuxindonesia: (1.196) @moonbowcloth_id: (1.196) perfect: (1.178) thanks: (1.162) hang: (1.157) fitness: (1.146) resort: (1.041) @megaekaputri_: (1.012) https://t.co/fcnojxrgld: (0.999) caterpillar: (0.999) park: (0.984) https://t.co/rwqvgoxm26: (0.965) \n",
      "\n",
      "Country = us\n",
      "http://t.co/ykkwnmzsir: (1.477) freakin: (1.352) like: (1.284) @ansleytolbert: (1.162) @savannerdd: (1.162) tonight: (1.140) going: (1.105) much: (1.069) can't: (1.035) home: (1.034) clue: (1.018) u: (1.000) beginning: (0.992) gym: (0.988) still: (0.976) believe: (0.952) boy: (0.936) @barackobama: (0.933) @thevoice_tf1: (0.933) #coffeespoonart: (0.933) \n",
      "\n",
      "Country = za\n",
      "https://t.co/igzld1xpw2: (1.704) https://t.co/2fsyuigben: (1.259) kosciiiiiiielny: (1.226) @zimtweets: (1.143) akubabuze: (1.143) https://t.co/upndf1513q: (1.118) @ferlo_mabkay: (1.118) makes: (1.100) man: (1.043) south: (1.000) africa: (1.000) goodnight: (0.983) consultation: (0.979) @giftnana7: (0.978) god: (0.974) de: (0.963) sir: (0.954) @enyawmm: (0.954) weekend: (0.950) making: (0.935) \n",
      "\n",
      "Classifier = Naive Bayes\n",
      "\n",
      "Country = au\n",
      "i'm: (-4.280) one: (-4.567) great: (-4.567) australia: (-4.749) melbourne: (-4.749) even: (-4.972) #mtvhottest: (-5.259) little: (-5.259) direction: (-5.259) #melbourne: (-5.259) victoria: (-5.663) come: (-5.663) visit: (-5.663) friends: (-5.663) christmas: (-5.663) cry: (-5.663) vca: (-5.663) done: (-5.663) tomorrow: (-5.663) pretty: (-5.663) \n",
      "\n",
      "Country = ca\n",
      "i'm: (-4.642) right: (-5.047) good: (-5.047) great: (-5.047) maybe: (-5.047) like: (-5.047) got: (-5.047) one: (-5.047) first: (-5.334) bed: (-5.334) thing: (-5.334) get: (-5.334) u: (-5.334) made: (-5.334) school: (-5.334) really: (-5.334) without: (-5.738) douglas: (-5.738) pair: (-5.738) next: (-5.738) \n",
      "\n",
      "Country = de\n",
      "love: (-4.054) posted: (-4.277) photo: (-4.564) enough: (-4.564) night: (-4.967) germany: (-4.967) come: (-4.967) week: (-4.967) i'm: (-4.967) almost: (-4.967) could: (-4.967) #rosegold: (-4.967) year: (-4.967) please: (-4.967) never: (-4.967) miss: (-4.967) unter: (-5.656) markus: (-5.656) #vegan: (-5.656) #urbangardening: (-5.656) \n",
      "\n",
      "Country = gb\n",
      "i'm: (-4.403) got: (-4.739) x: (-4.739) hope: (-4.962) new: (-5.248) fear: (-5.248) that's: (-5.248) wow: (-5.248) sure: (-5.248) know: (-5.248) time: (-5.248) need: (-5.248) right: (-5.248) beat: (-5.248) haha: (-5.248) xx: (-5.248) beautiful: (-5.248) plz: (-5.248) lol: (-5.652) get: (-5.652) \n",
      "\n",
      "Country = id\n",
      "pic: (-4.145) rt: (-4.433) i'm: (-4.615) donuts: (-5.124) coffee: (-5.124) hiks: (-5.124) cafe: (-5.124) others: (-5.124) shaw: (-5.528) hehhe: (-5.528) room: (-5.528) banget: (-5.528) flat: (-5.528) j.co: (-5.528) ga: (-5.528) senayan: (-5.528) bagus: (-5.528) one: (-5.528) lasagna: (-5.528) way: (-5.528) \n",
      "\n",
      "Country = my\n",
      "i'm: (-3.614) w: (-4.460) selangor: (-4.460) happy: (-4.642) jaya: (-4.642) birthday: (-4.865) thank: (-5.151) nk: (-5.151) maybe: (-5.151) subang: (-5.151) others: (-5.151) last: (-5.151) u: (-5.151) cendol: (-5.151) tu: (-5.151) petaling: (-5.151) aku: (-5.151) tp: (-5.555) today: (-5.555) leman: (-5.555) \n",
      "\n",
      "Country = ph\n",
      "follow: (-4.477) please: (-4.477) city: (-4.882) like: (-4.882) :d: (-4.882) back: (-4.882) im: (-4.882) manila: (-5.169) i'm: (-5.169) yesterday: (-5.169) us: (-5.169) good: (-5.169) lol: (-5.169) sm: (-5.169) may: (-5.169) make: (-5.169) come: (-5.572) ako: (-5.572) love: (-5.572) gonna: (-5.572) \n",
      "\n",
      "Country = sg\n",
      "singapore: (-4.101) see: (-4.793) w: (-4.793) i'm: (-4.793) park: (-4.793) pic: (-5.080) thanks: (-5.080) hotel: (-5.080) work: (-5.080) resort: (-5.080) fitness: (-5.080) use: (-5.080) good: (-5.080) others: (-5.080) people: (-5.483) east: (-5.483) @weichen_7: (-5.483) @wwindaruby: (-5.483) irwan: (-5.483) @ukhtirofie_: (-5.483) \n",
      "\n",
      "Country = us\n",
      "like: (-4.182) u: (-4.469) going: (-4.651) go: (-4.874) i'm: (-4.874) much: (-5.161) home: (-5.161) time: (-5.161) tonight: (-5.161) get: (-5.161) still: (-5.161) miss: (-5.161) lol: (-5.161) feel: (-5.161) boy: (-5.161) person: (-5.565) beginning: (-5.565) believe: (-5.565) ready: (-5.565) beer: (-5.565) \n",
      "\n",
      "Country = za\n",
      "day: (-4.511) happy: (-4.693) u: (-4.693) love: (-4.915) time: (-4.915) today: (-4.915) like: (-4.915) de: (-4.915) god: (-4.915) b: (-5.202) good: (-5.202) guys: (-5.202) makes: (-5.202) everything: (-5.202) south: (-5.202) africa: (-5.202) weekend: (-5.202) way: (-5.202) got: (-5.606) best: (-5.606) \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bachfischer/opt/anaconda3/envs/nlp/lib/python3.8/site-packages/sklearn/utils/deprecation.py:101: FutureWarning: Attribute coef_ was deprecated in version 0.24 and will be removed in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "# Your answer BEGINS HERE\n",
    "###\n",
    "def print_top20(vectorizer, clf, class_labels):\n",
    "    \"\"\"Prints features with the highest coefficient values per class\"\"\"\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        # Returns top-20 indices\n",
    "        top20 = np.argsort(clf.coef_[i])[-20:]\n",
    "        # Sort the array to get highest first\n",
    "        top20 = top20[::-1]\n",
    "        print(\"Country = \" + str(class_label))\n",
    "        for j in top20:    \n",
    "            print(\"%s: (%.3f)\" % (feature_names[j], clf.coef_[i][j]), end=' ')\n",
    "        print()\n",
    "        print()\n",
    "\n",
    "print(\"Classifier = Logistic Regression\")\n",
    "print()\n",
    "print_top20(vectorizer, LR_final_model, class_labels=LR_final_model.classes_)\n",
    "\n",
    "print(\"Classifier = Naive Bayes\")\n",
    "print()\n",
    "print_top20(vectorizer, NB_final_model, class_labels=NB_final_model.classes_)\n",
    "\n",
    "\n",
    "# Interesting observation - features differ heavily by country (every country has it's unique set of features!)\n",
    "\n",
    "###\n",
    "# Your answer ENDS HERE\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
